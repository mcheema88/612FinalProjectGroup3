{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812e210-0feb-45c4-9f93-84968a6019bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"612FinalProject\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77c6b6-ca75-43ae-8510-79f24c064ec0",
   "metadata": {},
   "source": [
    "#Cleaning and Combining Weather Data\n",
    "This step was perhaps the most energy consuming when it comes to our data apprehensions itself, and while we tried to prevent to get to the files in our database we had some manual work.\n",
    "\n",
    "https://acis.alberta.ca/weather-data-viewer.jsp  \n",
    "\n",
    "To access weather data you could choose a weather station, and then from there download the data you needed (daily, hourly ect), of your choice, this website however enforced a CAPTCHA which for our skillset prevented an automated request, what also was interesting was that because our AESO DATA had it's own specific \"regions\", we had to be diligent to choose a weather station that we believed existed within the defined AESO region.  \n",
    "\n",
    "We referenced this AESO MAP, which shows the boundaries of each Region that is used for our AESO Load Data.  \n",
    "https://www.arcgis.com/apps/View/index.html?appid=88859e0179b44b47b3e77b0b384a18a7  \n",
    "\n",
    "This was super helpful as our Weather Data also had an interactive map on the data page which showed the location of each weather station so through overlaying them, we could effectively choose a weather station that fell within the region (this map can be seen in the weather data link provided above).\n",
    "\n",
    "This still left with the problem that limitations of our Weather data download could only be for to 184 days (while this was not ideal, it does give a great opportunity to demonstrate our data parsing and combining skills which will be done below0.  \n",
    "\n",
    "So after identifying a station that fell into each of the AESO regions, we download 3 files for each station of HOURLY weather data which corresponded to 2023-11-01 - 2024-05-05,  2024-05-06 - 2024-10-01,  and finally 2024-10-02 - 2024-12-31, these dates are apart of the download file for our WEATHER DATA and together represent the timeline of our AESO LOAD data which is from 2023-11-01 - 2024-12-31 also HOURLY so this was inetntional so that of course we can match our data sets perfectly.  \n",
    "\n",
    "However because our decisions of Weather Data Station belonging to a AESO region was a manual decision with no clear way to automate, each weather station CSV was just renamed at the end by adding the Region of our Choice (numerical number) to the end of the CSV name, which is found in our data folder under WeatherDataRaw, as everything is untouched and represents 42 Weather Stations data (representing the 42 Aeso Regions) x 3 (as we had to download the weather data again in chunks of 3 for each station.  \n",
    "\n",
    "Below is our process of combining the 3 sets of data for the station into 1 dataframe for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c43c68-4890-4ef0-baff-8538c79e71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "weather_dir = \"../data/WeatherDataRaw\"\n",
    "\n",
    "# Find all the ACIS files\n",
    "weather_files = glob.glob(os.path.join(weather_dir, \"ACISHourlyData-*.csv\"))\n",
    "print(f\"Found {len(weather_files)} weather CSV files\")\n",
    "\n",
    "# 1) Group file paths by station code (last piece before .csv)\n",
    "files_by_code = {}\n",
    "\n",
    "for path in weather_files:\n",
    "    fname = os.path.basename(path)             # An example of ACISHourlyData-20231101-20240505-4.csv\n",
    "    name_no_ext = fname[:-4]                   # strip \".csv\"\n",
    "    parts = name_no_ext.split(\"-\")             # ['ACISHourlyData','20231101','20240505','4']\n",
    "\n",
    "    start_date = parts[1]                      # '20231101'\n",
    "    code = parts[-1]                           # '4', '6', '13', ...\n",
    "\n",
    "    files_by_code.setdefault(code, []).append((start_date, path))\n",
    "\n",
    "print(\"Codes found:\", sorted(files_by_code.keys()))\n",
    "\n",
    "# 2) For each code, read its files in date order and make a combined DataFrame\n",
    "date_col = \"Date (Local Standard Time)\"  # This was the column in the downloaded weather data which held the date and time (again\n",
    "#this was collected hourly so it has a date and time which is standard, if we to make it one so we can sort\n",
    "\n",
    "for code, lst in files_by_code.items():\n",
    "    lst_sorted = sorted(lst, key=lambda t: t[0])  #This is the sorting of it by data so our combined data frame\n",
    "    #is in order of date which should start at November 1 2023 at 0:00 and should end at Decmeber 31st 23:00\n",
    "\n",
    "    dfs = []\n",
    "    for start_date, path in lst_sorted:\n",
    "        print(f\"Reading code {code} file:\", os.path.basename(path))\n",
    "        df = pd.read_csv(path, encoding=\"latin1\")  # encoding fix for UnicodeDecodeError -> Reference to ChatGPT to help us debug we had an error here for a while\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Steo to ensure that it's correctly order it's ordered by date/time, JUST A CHECK to drop for each time\n",
    "    if date_col in combined.columns:\n",
    "        combined[date_col] = pd.to_datetime(combined[date_col])\n",
    "        combined = combined.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    #This is now our name for our combined file which we are just calling combined_data and the respective region\n",
    "    var_name = f\"combined_data_{code}\"\n",
    "    globals()[var_name] = combined\n",
    "\n",
    "    #a check to ensure the rows are correct, ideally each combined_weather_station data should be the same length\n",
    "    #which would ensure all the downloaded data was correct with dates, and there are no mistakes in our combining process\n",
    "    #this is still straightforward\n",
    "    print(f\"Created variable '{var_name}' with {len(combined)} rows\")\n",
    "\n",
    "print(\"Done combining all station codes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e48aa-a29e-4e80-9e38-d2a5a412dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_4.head()    # Medicine Hat Weather Station (also the region name in AESO)\n",
    "#looks good time to move one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496c9cf-acf4-431e-8ff8-be2d26ce0db5",
   "metadata": {},
   "source": [
    "Okay this all working beautiful as you can see above  \n",
    "\n",
    "This is exactly what we wanted as we have found all the Regions (and codes correctly), which means our data looked good, and combined data set has the same number of Rows of 10247, which means their are no rows MISSING! And from our output which we xtended the data is merged in correct order, so we have a complete set for each station from November 1st 2023 to December 31st 2024 for each weather station (again this alligns beautifully, with our AESO data which we will work on next so we can eventually combine both after that is cleaned (and combined to our Urban and Rural classifier). Awesome, perhaps there may be NANS/NULL ect for our Air Temp. Inst and Precip.(mm), but we can check for that no problem.  \n",
    "\n",
    "Lets now make all of our data into one complete data set so that we have our base for PIPELINE to be executed on it, as we learned in 612."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5b3eb-4774-4002-bb37-37d86e65f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Using the exact loop that I printed above which gave me this list - we also of course have hard copies everywhere.\n",
    "codes = ['4', '6', '13', '17', '18', '19', '20', '21', '22', '23',\n",
    "         '24', '25', '26', '27', '28', '29', '30', '31', '32', '33',\n",
    "         '34', '35', '36', '37', '38', '39', '40', '42', '43', '44',\n",
    "         '45', '46', '47', '48', '49', '52', '53', '54', '55', '56',\n",
    "         '57', '60']\n",
    "\n",
    "frames = []\n",
    "\n",
    "for code in codes:\n",
    "    df = globals()[f\"combined_data_{code}\"].copy()\n",
    "\n",
    "    # attach area code\n",
    "    df[\"area_code\"] = int(code)\n",
    "\n",
    "    # ----- find the precipitation column, if any -----\n",
    "    precip_candidates = [\n",
    "        \"Precip. (mm)\",\n",
    "        \"Precip.(mm)\",\n",
    "        \"Precip. Amount (mm)\",\n",
    "    ]\n",
    "    precip_col = None\n",
    "    for c in precip_candidates:\n",
    "        if c in df.columns:\n",
    "            precip_col = c\n",
    "            break\n",
    "\n",
    "    # build rename map\n",
    "    rename_map = {\n",
    "        \"Date (Local Standard Time)\": \"timestamp\",\n",
    "        \"Air Temp. Inst. (Â°C)\": \"temp_c\",\n",
    "    }\n",
    "    if precip_col is not None:\n",
    "        rename_map[precip_col] = \"precip_mm\"\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # if still no precip_mm, create it (all NaN or 0.0 depending on what you want)\n",
    "    if \"precip_mm\" not in df.columns:\n",
    "        df[\"precip_mm\"] = np.nan     # or 0.0 if you prefer\n",
    "\n",
    "    # keep only modeling columns\n",
    "    frames.append(df[[\"area_code\", \"timestamp\", \"temp_c\", \"precip_mm\"]])\n",
    "\n",
    "# combine all stations\n",
    "weather_all_pd = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# ensure timestamp is datetime\n",
    "weather_all_pd[\"timestamp\"] = pd.to_datetime(weather_all_pd[\"timestamp\"])\n",
    "\n",
    "print(\"Area codes in weather:\", sorted(weather_all_pd[\"area_code\"].unique()))\n",
    "print(weather_all_pd.head())\n",
    "\n",
    "# Now need to get it all back into the CSV so we can load it into our SPARK\n",
    "weather_all_pd.to_csv(\"../data/weather_all_areas_hourly.csv\", index=False)\n",
    "print(\"Saved ../data/weather_all_areas_hourly.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9876cd-85e3-48f5-8330-d631feeaedb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4ccb8-e0ee-4385-99f3-12f15773c683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414f9e6-6193-4b51-8c12-9d12b469f79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
