{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Fix Spark Python environment mismatch\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# Hadoop + winutils\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop-3.3.6\"\n",
    "os.environ[\"PATH\"] += \";C:\\\\hadoop-3.3.6\\\\bin\"\n",
    "\n",
    "# Java 17 (Spark 4 requirement)\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-17\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\\\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Fix Windows Hadoop user identity error\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for PySpark MLlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler as SparkStandardScaler\n",
    "from pyspark.ml.regression import (\n",
    "    LinearRegression, DecisionTreeRegressor, RandomForestRegressor, \n",
    "    GBTRegressor, GeneralizedLinearRegression\n",
    ")\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"All PySpark MLlib libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c65109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session (if not already available)\n",
    "try:\n",
    "    spark\n",
    "    print(\"Using existing Spark session\")\n",
    "except NameError:\n",
    "    print(\"Creating new Spark session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MLModelTraining\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Spark session created successfully\")\n",
    "\n",
    "# Load the aeso_weather_Final dataset from parquet file\n",
    "print(\"\\nLoading aeso_weather_Final dataset from parquet file...\")\n",
    "\n",
    "parquet_path = \"../data/aeso_weather_Final.parquet\"\n",
    "\n",
    "try:\n",
    "    # Load the dataset from parquet file\n",
    "    df_spark = spark.read.parquet(parquet_path)\n",
    "    \n",
    "    print(f\"Successfully loaded dataset from {parquet_path}\")\n",
    "    print(f\"Dataset shape: {df_spark.count()} rows, {len(df_spark.columns)} columns\")\n",
    "    print(\"Dataset columns:\", df_spark.columns)\n",
    "    \n",
    "    # Show sample of the data\n",
    "    print(\"\\nSample of loaded data:\")\n",
    "    df_spark.show(5)\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset from parquet: {str(e)}\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"1. The AESOLoadDataCleaningAndFraming notebook has been run\")\n",
    "    print(\"2. The aeso_weather_Final dataset has been saved to parquet\")\n",
    "    print(f\"3. The parquet file exists at: {parquet_path}\")\n",
    "    raise Exception(\"Could not load aeso_weather_Final dataset from parquet file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure and prepare for ML\n",
    "print(\"Exploring dataset structure with PySpark...\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df_spark.count()} rows, {len(df_spark.columns)} columns\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "for col in df_spark.columns:\n",
    "    missing_count = df_spark.filter(F.col(col).isNull()).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"{col}: {missing_count}\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_spark.show(5)\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Check for the target variable and get statistics\n",
    "if 'load_mw' in df_spark.columns:\n",
    "    print(f\"\\nTarget variable (load_mw) statistics:\")\n",
    "    df_spark.select('load_mw').describe().show()\n",
    "    \n",
    "    # For visualization, sample some data to pandas\n",
    "    load_sample = df_spark.select('load_mw').sample(0.1, seed=42).toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(load_sample['load_mw'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Energy Load (MW) - Sample')\n",
    "    plt.xlabel('Load (MW)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Warning: 'load_mw' column not found in dataset!\")\n",
    "    print(\"Available columns:\", df_spark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PySpark MLlib\n",
    "print(\"Preparing data for PySpark MLlib...\")\n",
    "\n",
    "# The aeso_weather_Final should have these columns based on the framing notebook:\n",
    "# - Numeric: temp_c, precip_mm\n",
    "# - Categorical (encoded): region_typeEncoder, seasonEncoder, hourEncoder, etc.\n",
    "# - Target: load_mw\n",
    "# - Features vector: features (already assembled)\n",
    "\n",
    "# Check if features vector already exists\n",
    "if 'features' in df_spark.columns:\n",
    "    print(\"Found existing 'features' vector column - will use this for ML\")\n",
    "    ml_df = df_spark.select('features', 'load_mw')\n",
    "    \n",
    "else:\n",
    "    print(\"Creating features vector from individual columns...\")\n",
    "    \n",
    "    # Find numeric columns (original numeric features)\n",
    "    numeric_features = ['temp_c', 'precip_mm']\n",
    "    \n",
    "    # Find encoded categorical features (those ending with 'Encoder')\n",
    "    encoded_features = [col for col in df_spark.columns if col.endswith('Encoder')]\n",
    "    \n",
    "    print(f\"Numeric features: {numeric_features}\")\n",
    "    print(f\"Encoded categorical features: {encoded_features}\")\n",
    "    \n",
    "    # Combine all feature columns\n",
    "    all_features = numeric_features + encoded_features\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    existing_features = [col for col in all_features if col in df_spark.columns]\n",
    "    print(f\"Using features: {existing_features}\")\n",
    "    \n",
    "    # Create VectorAssembler to combine features\n",
    "    assembler = VectorAssembler(inputCols=existing_features, outputCol=\"features\")\n",
    "    ml_df = assembler.transform(df_spark).select('features', 'load_mw')\n",
    "\n",
    "print(\"\\nML DataFrame structure:\")\n",
    "ml_df.printSchema()\n",
    "print(f\"ML DataFrame count: {ml_df.count()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of ML data:\")\n",
    "ml_df.show(5, truncate=False)\n",
    "\n",
    "# Handle missing values if any\n",
    "missing_features = ml_df.filter(F.col('features').isNull()).count()\n",
    "missing_target = ml_df.filter(F.col('load_mw').isNull()).count()\n",
    "\n",
    "print(f\"\\nMissing features: {missing_features}\")\n",
    "print(f\"Missing target values: {missing_target}\")\n",
    "\n",
    "# Remove rows with missing values\n",
    "if missing_features > 0 or missing_target > 0:\n",
    "    print(\"Removing rows with missing values...\")\n",
    "    ml_df = ml_df.filter(F.col('features').isNotNull() & F.col('load_mw').isNotNull())\n",
    "    print(f\"After removing missing values: {ml_df.count()} rows\")\n",
    "\n",
    "print(\"Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a427890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing using PySpark\n",
    "print(\"Splitting data for training and testing...\")\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_df.count()} samples\")\n",
    "print(f\"Test set size: {test_df.count()} samples\")\n",
    "print(f\"Total samples: {ml_df.count()}\")\n",
    "\n",
    "# Cache the datasets for better performance\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "print(\"Data splitting completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0904f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train multiple PySpark MLlib models\n",
    "print(\"Training multiple PySpark MLlib models...\")\n",
    "\n",
    "# Initialize PySpark ML models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(featuresCol='features', labelCol='load_mw', predictionCol='prediction'),\n",
    "    'Decision Tree': DecisionTreeRegressor(featuresCol='features', labelCol='load_mw', predictionCol='prediction', maxDepth=10, seed=42),\n",
    "    'Random Forest': RandomForestRegressor(featuresCol='features', labelCol='load_mw', predictionCol='prediction', numTrees=100, seed=42),\n",
    "    'Gradient Boosting': GBTRegressor(featuresCol='features', labelCol='load_mw', predictionCol='prediction', maxIter=100, seed=42),\n",
    "    'GLM - Gaussian': GeneralizedLinearRegression(featuresCol='features', labelCol='load_mw', predictionCol='prediction', family='gaussian'),\n",
    "    'GLM - Poisson': GeneralizedLinearRegression(featuresCol='features', labelCol='load_mw', predictionCol='prediction', family='poisson')\n",
    "}\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluators = {\n",
    "    'RMSE': RegressionEvaluator(labelCol='load_mw', predictionCol='prediction', metricName='rmse'),\n",
    "    'MAE': RegressionEvaluator(labelCol='load_mw', predictionCol='prediction', metricName='mae'),\n",
    "    'R2': RegressionEvaluator(labelCol='load_mw', predictionCol='prediction', metricName='r2')\n",
    "}\n",
    "\n",
    "# Train models and collect results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        fitted_model = model.fit(train_df)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        predictions = fitted_model.transform(test_df)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        model_results = {}\n",
    "        for metric_name, evaluator in evaluators.items():\n",
    "            score = evaluator.evaluate(predictions)\n",
    "            model_results[metric_name] = score\n",
    "        \n",
    "        results[name] = model_results\n",
    "        trained_models[name] = fitted_model\n",
    "        \n",
    "        print(f\"{name} - R2: {model_results['R2']:.4f}, RMSE: {model_results['RMSE']:.2f}, MAE: {model_results['MAE']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {str(e)}\")\n",
    "        results[name] = {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison\n",
    "print(\"Creating model comparison analysis...\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "results_df_sorted = results_df.sort_values('R2', ascending=False)\n",
    "\n",
    "print(\"Model Performance Ranking:\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df_sorted.to_string())\n",
    "\n",
    "# Visualization of model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# R² Score comparison\n",
    "model_names = results_df_sorted.index\n",
    "r2_scores = results_df_sorted['R2']\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "bars1 = axes[0,0].barh(model_names, r2_scores, color=colors)\n",
    "axes[0,0].set_title('Model Performance - R² Score')\n",
    "axes[0,0].set_xlabel('R² Score')\n",
    "axes[0,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars1, r2_scores):\n",
    "    if not np.isnan(score):\n",
    "        axes[0,0].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{score:.3f}', va='center')\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_scores = results_df_sorted['RMSE']\n",
    "bars2 = axes[0,1].barh(model_names, rmse_scores, color=colors)\n",
    "axes[0,1].set_title('Model Performance - RMSE (Lower is Better)')\n",
    "axes[0,1].set_xlabel('RMSE')\n",
    "axes[0,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "mae_scores = results_df_sorted['MAE']\n",
    "bars3 = axes[1,0].barh(model_names, mae_scores, color=colors)\n",
    "axes[1,0].set_title('Model Performance - MAE (Lower is Better)')\n",
    "axes[1,0].set_xlabel('MAE')\n",
    "axes[1,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Prediction vs Actual for best model\n",
    "best_model_name = results_df_sorted.index[0]\n",
    "if best_model_name in trained_models:\n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_predictions = best_model.transform(test_df)\n",
    "    \n",
    "    # Sample for visualization\n",
    "    pred_sample = best_predictions.select('load_mw', 'prediction').sample(0.2, seed=42).toPandas()\n",
    "    \n",
    "    axes[1,1].scatter(pred_sample['load_mw'], pred_sample['prediction'], alpha=0.5)\n",
    "    axes[1,1].plot([pred_sample['load_mw'].min(), pred_sample['load_mw'].max()], \n",
    "                   [pred_sample['load_mw'].min(), pred_sample['load_mw'].max()], 'r--', lw=2)\n",
    "    axes[1,1].set_title(f'Best Model ({best_model_name}) - Predicted vs Actual')\n",
    "    axes[1,1].set_xlabel('Actual Energy Load (MW)')\n",
    "    axes[1,1].set_ylabel('Predicted Energy Load (MW)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance from tree-based models\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "feature_importance = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Get feature importances (available for tree-based models)\n",
    "        if hasattr(model, 'featureImportances'):\n",
    "            importances = model.featureImportances.toArray()\n",
    "            \n",
    "            # Create feature names (limited information from vector)\n",
    "            feature_names = [f'Feature_{j}' for j in range(len(importances))]\n",
    "            \n",
    "            # Sort by importance\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Get top features (limit to top 15 for readability)\n",
    "            top_n = min(15, len(importances))\n",
    "            top_indices = indices[:top_n]\n",
    "            top_importances = importances[top_indices]\n",
    "            top_feature_names = [feature_names[j] for j in top_indices]\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].barh(range(len(top_importances)), top_importances)\n",
    "            axes[i].set_title(f'{model_name} - Feature Importance')\n",
    "            axes[i].set_yticks(range(len(top_importances)))\n",
    "            axes[i].set_yticklabels(top_feature_names)\n",
    "            axes[i].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Store for summary\n",
    "            feature_importance[model_name] = dict(zip(top_feature_names, top_importances))\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f'{model_name}\\nFeature importance\\nnot available', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f'{model_name} - Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 5 features for each tree model\n",
    "if feature_importance:\n",
    "    print(\"\\nTop 5 Most Important Features by Model:\")\n",
    "    print(\"=\" * 60)\n",
    "    for model_name, features in feature_importance.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for i, (feature, importance) in enumerate(list(features.items())[:5]):\n",
    "            print(f\"  {i+1}. {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo feature importance information available from the trained models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation and hyperparameter tuning for best model\n",
    "print(\"Performing cross-validation and hyperparameter tuning...\")\n",
    "\n",
    "# Get the best performing model\n",
    "best_model_name = results_df_sorted.index[0]\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Perform cross-validation for Random Forest (if it's among the top performers)\n",
    "if 'Random Forest' in results and not np.isnan(results['Random Forest']['R2']):\n",
    "    print(\"\\nPerforming cross-validation and hyperparameter tuning for Random Forest...\")\n",
    "    \n",
    "    # Create parameter grid\n",
    "    rf = RandomForestRegressor(featuresCol='features', labelCol='load_mw', seed=42)\n",
    "    \n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(rf.numTrees, [50, 100, 150])\n",
    "                 .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "                 .build())\n",
    "    \n",
    "    # Create cross validator\n",
    "    evaluator = RegressionEvaluator(labelCol='load_mw', predictionCol='prediction', metricName='r2')\n",
    "    crossval = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=evaluator,\n",
    "                             numFolds=3)\n",
    "    \n",
    "    # Fit cross validator\n",
    "    print(\"Running cross-validation... (this may take a while)\")\n",
    "    cvModel = crossval.fit(train_df)\n",
    "    \n",
    "    # Get best model\n",
    "    bestModel = cvModel.bestModel\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    predictions = bestModel.transform(test_df)\n",
    "    tuned_r2 = evaluator.evaluate(predictions)\n",
    "    tuned_rmse = RegressionEvaluator(labelCol='load_mw', predictionCol='prediction', metricName='rmse').evaluate(predictions)\n",
    "    tuned_mae = RegressionEvaluator(labelCol='load_mw', predictionCol='prediction', metricName='mae').evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\nTuned Random Forest Performance:\")\n",
    "    print(f\"R²: {tuned_r2:.4f}\")\n",
    "    print(f\"RMSE: {tuned_rmse:.2f}\")\n",
    "    print(f\"MAE: {tuned_mae:.2f}\")\n",
    "    print(f\"Best parameters: numTrees={bestModel.getNumTrees}, maxDepth={bestModel.getMaxDepth()}\")\n",
    "    \n",
    "    # Update results with tuned model\n",
    "    results['Tuned Random Forest'] = {\n",
    "        'R2': tuned_r2,\n",
    "        'RMSE': tuned_rmse,\n",
    "        'MAE': tuned_mae\n",
    "    }\n",
    "    trained_models['Tuned Random Forest'] = bestModel\n",
    "\n",
    "else:\n",
    "    print(\"Random Forest not available or didn't perform well enough for hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f176c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analysis: Model interpretability and insights\n",
    "print(\"Analyzing model predictions and insights...\")\n",
    "\n",
    "# Get the overall best model\n",
    "final_results = pd.DataFrame(results).T.sort_values('R2', ascending=False)\n",
    "best_model_name = final_results.index[0]\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Using {best_model_name} for detailed analysis...\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = best_model.transform(test_df)\n",
    "\n",
    "# Calculate residuals\n",
    "test_with_residuals = test_predictions.withColumn('residual', F.col('load_mw') - F.col('prediction'))\n",
    "\n",
    "print(\"Residual Analysis:\")\n",
    "test_with_residuals.select('residual').describe().show()\n",
    "\n",
    "# Sample data for visualization\n",
    "sample_data = test_with_residuals.select('load_mw', 'prediction', 'residual').sample(0.3, seed=42).toPandas()\n",
    "\n",
    "# Create residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(sample_data['prediction'], sample_data['residual'], alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_title('Residuals vs Predicted Values')\n",
    "axes[0].set_xlabel('Predicted Load (MW)')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1].hist(sample_data['residual'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for residuals normality\n",
    "from scipy import stats\n",
    "stats.probplot(sample_data['residual'], dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot of Residuals')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate prediction intervals\n",
    "residual_std = sample_data['residual'].std()\n",
    "print(f\"\\nPrediction Analysis:\")\n",
    "print(f\"Residual Standard Deviation: {residual_std:.2f} MW\")\n",
    "print(f\"95% Prediction Interval: ± {1.96 * residual_std:.2f} MW\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(f\"\\nExample Predictions from {best_model_name}:\")\n",
    "example_predictions = test_predictions.select('load_mw', 'prediction').limit(10).toPandas()\n",
    "example_predictions['error'] = example_predictions['load_mw'] - example_predictions['prediction']\n",
    "example_predictions['error_pct'] = (example_predictions['error'] / example_predictions['load_mw'] * 100).round(2)\n",
    "print(example_predictions.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and insights\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON SUMMARY - PySpark MLlib\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create final results DataFrame\n",
    "final_results = pd.DataFrame(results).T\n",
    "final_results = final_results.sort_values('R2', ascending=False, na_position='last')\n",
    "\n",
    "print(\"\\nFinal Model Rankings:\")\n",
    "print(final_results.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get best performing model\n",
    "valid_results = final_results.dropna(subset=['R2'])\n",
    "if len(valid_results) > 0:\n",
    "    best_model_final = valid_results.index[0]\n",
    "    best_r2 = valid_results.loc[best_model_final, 'R2']\n",
    "    best_rmse = valid_results.loc[best_model_final, 'RMSE']\n",
    "    best_mae = valid_results.loc[best_model_final, 'MAE']\n",
    "    \n",
    "    print(f\"\\n1. BEST PERFORMING MODEL: {best_model_final}\")\n",
    "    print(f\"   - R² Score: {best_r2:.4f}\")\n",
    "    print(f\"   - RMSE: {best_rmse:.2f} MW\")\n",
    "    print(f\"   - MAE: {best_mae:.2f} MW\")\n",
    "    print(f\"   - Explains {best_r2*100:.1f}% of the variance in energy load\")\n",
    "    \n",
    "    print(f\"\\n2. PYSPARK MLLIB MODEL COMPARISON:\")\n",
    "    # Compare model types\n",
    "    tree_models_in_results = [m for m in ['Decision Tree', 'Random Forest', 'Gradient Boosting'] if m in valid_results.index]\n",
    "    linear_models_in_results = [m for m in ['Linear Regression', 'GLM - Gaussian'] if m in valid_results.index]\n",
    "    \n",
    "    if tree_models_in_results:\n",
    "        tree_models_avg = valid_results.loc[tree_models_in_results, 'R2'].mean()\n",
    "        print(f\"   - Tree-based models average R²: {tree_models_avg:.4f}\")\n",
    "        print(f\"   - Best tree model: {valid_results.loc[tree_models_in_results].iloc[0].name}\")\n",
    "    \n",
    "    if linear_models_in_results:\n",
    "        linear_models_avg = valid_results.loc[linear_models_in_results, 'R2'].mean()\n",
    "        print(f\"   - Linear models average R²: {linear_models_avg:.4f}\")\n",
    "        print(f\"   - Best linear model: {valid_results.loc[linear_models_in_results].iloc[0].name}\")\n",
    "    \n",
    "    if tree_models_in_results and linear_models_in_results:\n",
    "        if tree_models_avg > linear_models_avg:\n",
    "            print(\"   - Tree-based models outperform linear models in this dataset\")\n",
    "        else:\n",
    "            print(\"   - Linear models perform competitively with tree-based models\")\n",
    "    \n",
    "    print(f\"\\n3. DATA SCALABILITY WITH PYSPARK:\")\n",
    "    print(f\"   - Successfully processed {ml_df.count():,} data points with PySpark\")\n",
    "    print(\"   - MLlib handles large-scale data efficiently\")\n",
    "    print(\"   - Models can scale to larger datasets without memory issues\")\n",
    "    \n",
    "    print(f\"\\n4. FEATURE IMPORTANCE INSIGHTS:\")\n",
    "    if feature_importance:\n",
    "        # Get most common top features across models\n",
    "        all_features = []\n",
    "        for model_features in feature_importance.values():\n",
    "            all_features.extend(list(model_features.keys())[:3])\n",
    "        \n",
    "        from collections import Counter\n",
    "        feature_counts = Counter(all_features)\n",
    "        print(\"   Most important features across tree-based models:\")\n",
    "        for feature, count in feature_counts.most_common(5):\n",
    "            print(f\"   - {feature}: mentioned in {count} models\")\n",
    "    else:\n",
    "        print(\"   Feature importance analysis was limited due to vector format\")\n",
    "        print(\"   Consider using the original feature columns for more detailed analysis\")\n",
    "    \n",
    "    print(f\"\\n5. PERFORMANCE ASSESSMENT:\")\n",
    "    if best_r2 > 0.9:\n",
    "        print(\"   - Excellent model performance (R² > 0.9)\")\n",
    "        print(\"   - Model predictions are highly reliable\")\n",
    "    elif best_r2 > 0.8:\n",
    "        print(\"   - Good model performance (R² > 0.8)\")\n",
    "        print(\"   - Model provides reliable predictions with some uncertainty\")\n",
    "    elif best_r2 > 0.7:\n",
    "        print(\"   - Decent model performance (R² > 0.7)\")\n",
    "        print(\"   - Model captures major patterns but has room for improvement\")\n",
    "    else:\n",
    "        print(\"   - Model performance has significant room for improvement\")\n",
    "        print(\"   - Consider feature engineering or collecting more data\")\n",
    "    \n",
    "    print(f\"\\n6. SPARK MLLIB ADVANTAGES:\")\n",
    "    print(\"   - Distributed computing capability for large datasets\")\n",
    "    print(\"   - Built-in feature processing and transformation\")\n",
    "    print(\"   - Seamless integration with Spark data processing pipeline\")\n",
    "    print(\"   - Scalable hyperparameter tuning with CrossValidator\")\n",
    "    \n",
    "    print(f\"\\n7. RECOMMENDATIONS:\")\n",
    "    print(f\"   - Deploy {best_model_final} for production energy load predictions\")\n",
    "    print(f\"   - Expected prediction accuracy: ±{best_rmse:.1f} MW (RMSE)\")\n",
    "    print(\"   - Use Spark's batch prediction capabilities for real-time inference\")\n",
    "    \n",
    "    if 'Tuned Random Forest' in results and not np.isnan(results['Tuned Random Forest']['R2']):\n",
    "        tuned_r2 = results['Tuned Random Forest']['R2']\n",
    "        if tuned_r2 > best_r2:\n",
    "            print(\"   - Consider using tuned Random Forest as it shows improved performance\")\n",
    "    \n",
    "    if best_r2 < 0.8:\n",
    "        print(\"   - Recommendations for improvement:\")\n",
    "        print(\"     * Engineer more weather-related features (humidity, wind speed)\")\n",
    "        print(\"     * Include lagged features (previous hours/days)\")\n",
    "        print(\"     * Add seasonal and holiday indicators\")\n",
    "        print(\"     * Consider ensemble methods combining multiple algorithms\")\n",
    "    \n",
    "    print(\"\\n   - Dataset and processing summary:\")\n",
    "    print(f\"     * Processed {ml_df.count():,} hourly energy load records\")\n",
    "    print(f\"     * Features derived from weather and temporal data\")\n",
    "    print(f\"     * Target variable range: Weather-dependent energy demand\")\n",
    "    print(\"     * Successfully leveraged Spark's distributed computing\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid model results found. Check data and model training process.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PYSPARK MLLIB ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis analysis used PySpark MLlib with the aeso_weather_Final dataset.\")\n",
    "print(\"Models were trained using distributed computing for scalability.\")\n",
    "print(\"The approach demonstrates enterprise-ready machine learning for energy forecasting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
