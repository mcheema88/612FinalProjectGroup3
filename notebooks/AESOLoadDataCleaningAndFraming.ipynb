{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe0c0c5-89f0-44bf-af3a-3a7c0448b3ef",
   "metadata": {},
   "source": [
    "AESO DATA CLEANING AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f828c907-79e0-4e19-9668-f22bcbf60625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combination of imports taken from my ENSF 612 Assignment 2, 3 and Midterm.\n",
    "#Added a few as datetime was specficially needed for the context of this assignment.\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, Tokenizer, StopWordsRemover,\n",
    "    HashingTF, IDF, VectorAssembler, StandardScaler\n",
    ")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98ec6a-0195-4a3a-828b-2b027ee72eb2",
   "metadata": {},
   "source": [
    "As we learned in 612, we now want to use SPARK DATAFRAMES to transform our AESO LOAD DATA  \n",
    "Now that we have already done our Weather Data (using pandas), we decided to take our AESO Load Data, which has columns for each region and than the date and hour as the first column populated and than for each column of the Aeso region has the load amount, ideally we want to be able to join that to the date of our Weather data of each region for that time (for the two features of Temprature and Precipitation) and the of course our Rural and Urban diagnostics for each region which we also created a manual Excel of which we got from research (link provided in that section).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33d28f-9c4e-436e-abc2-d2e86826a2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4ac8289-52dc-4d97-a82b-a583e7bd6e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 07:30:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"612FinalProject\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c564a9a-210a-4035-bf45-be5472d108bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/behzad/612FinalProjectGroup3/notebooks\n",
      "root\n",
      " |-- DT_MST: timestamp (nullable = true)\n",
      " |-- AREA13: double (nullable = true)\n",
      " |-- AREA17: double (nullable = true)\n",
      " |-- AREA18: double (nullable = true)\n",
      " |-- AREA19: double (nullable = true)\n",
      " |-- AREA20: double (nullable = true)\n",
      " |-- AREA21: double (nullable = true)\n",
      " |-- AREA22: double (nullable = true)\n",
      " |-- AREA23: double (nullable = true)\n",
      " |-- AREA24: double (nullable = true)\n",
      " |-- AREA25: double (nullable = true)\n",
      " |-- AREA26: double (nullable = true)\n",
      " |-- AREA27: double (nullable = true)\n",
      " |-- AREA28: double (nullable = true)\n",
      " |-- AREA29: double (nullable = true)\n",
      " |-- AREA30: double (nullable = true)\n",
      " |-- AREA31: double (nullable = true)\n",
      " |-- AREA32: double (nullable = true)\n",
      " |-- AREA33: double (nullable = true)\n",
      " |-- AREA34: double (nullable = true)\n",
      " |-- AREA35: double (nullable = true)\n",
      " |-- AREA36: double (nullable = true)\n",
      " |-- AREA37: double (nullable = true)\n",
      " |-- AREA38: double (nullable = true)\n",
      " |-- AREA39: double (nullable = true)\n",
      " |-- AREA4: double (nullable = true)\n",
      " |-- AREA40: double (nullable = true)\n",
      " |-- AREA42: double (nullable = true)\n",
      " |-- AREA43: double (nullable = true)\n",
      " |-- AREA44: double (nullable = true)\n",
      " |-- AREA45: double (nullable = true)\n",
      " |-- AREA46: double (nullable = true)\n",
      " |-- AREA47: double (nullable = true)\n",
      " |-- AREA48: double (nullable = true)\n",
      " |-- AREA49: double (nullable = true)\n",
      " |-- AREA52: double (nullable = true)\n",
      " |-- AREA53: double (nullable = true)\n",
      " |-- AREA54: double (nullable = true)\n",
      " |-- AREA55: double (nullable = true)\n",
      " |-- AREA56: double (nullable = true)\n",
      " |-- AREA57: double (nullable = true)\n",
      " |-- AREA6: double (nullable = true)\n",
      " |-- AREA60: double (nullable = true)\n",
      " |-- Calgary: double (nullable = true)\n",
      " |-- Central: double (nullable = true)\n",
      " |-- Edmonton: double (nullable = true)\n",
      " |-- Northeast: double (nullable = true)\n",
      " |-- Northwest: double (nullable = true)\n",
      " |-- South: double (nullable = true)\n",
      "\n",
      "+-------------------+-----------+--------+----------+--------+---------+--------+--------+--------+----------+-----------+----------+-----------+-----------+----------+-----------+---------+-----------+-----------+---------+-----------+-------+----------+----------+----------+----------+-----------+-----------+--------+-----------+---------+----------+----------+-----------+--------+----------+----------+-----------+----------+---------+----------+-----------+------------+-----------+------------+------------+------------+-----------+-----------+\n",
      "|             DT_MST|     AREA13|  AREA17|    AREA18|  AREA19|   AREA20|  AREA21|  AREA22|  AREA23|    AREA24|     AREA25|    AREA26|     AREA27|     AREA28|    AREA29|     AREA30|   AREA31|     AREA32|     AREA33|   AREA34|     AREA35| AREA36|    AREA37|    AREA38|    AREA39|     AREA4|     AREA40|     AREA42|  AREA43|     AREA44|   AREA45|    AREA46|    AREA47|     AREA48|  AREA49|    AREA52|    AREA53|     AREA54|    AREA55|   AREA56|    AREA57|      AREA6|      AREA60|    Calgary|     Central|    Edmonton|   Northeast|  Northwest|      South|\n",
      "+-------------------+-----------+--------+----------+--------+---------+--------+--------+--------+----------+-----------+----------+-----------+-----------+----------+-----------+---------+-----------+-----------+---------+-----------+-------+----------+----------+----------+----------+-----------+-----------+--------+-----------+---------+----------+----------+-----------+--------+----------+----------+-----------+----------+---------+----------+-----------+------------+-----------+------------+------------+------------+-----------+-----------+\n",
      "|2023-11-01 00:00:00| 100.173998|5.166425|37.6438593|71.10166|275.33804|45.18537|57.69576|18.43956|85.4734143|786.5661492|84.6379887|105.2415026|118.8627063|84.1644369|111.5552975|93.982206|115.2856121|425.7143897|0.9608802|260.8927518|5.27676|93.3207627|74.8424413|52.0821192|26.7341841| 96.2287477|125.3596243|10.47728|137.9625539|68.734615|43.1460533|62.7906296|167.4516927|9.393875|66.8567559|11.7748728|105.7738202|27.7163967|60.840623|59.0846028| 879.926368|1123.2788268|939.0109708|1203.6180133|1313.4897805|1317.5220415|680.6820773|738.8127292|\n",
      "|2023-11-01 01:00:00| 95.4448056| 5.08886|36.7094137| 71.0077|273.19208|40.36107|55.86708| 18.3546|84.6385896|771.0848474|81.8874129|103.7235509|114.9490187|84.6785141|112.9104477| 92.18924| 116.897898|425.6909897|0.9635216|258.2517431| 5.2236|92.2658773|74.2311014|51.9929935|28.0769453| 97.8291766|126.0469785|11.08044|137.8524557|68.086786|42.3406256|62.5837744| 168.734206|9.318267|67.0461097|12.6719149| 105.327671|27.5511736|60.214221|57.8353193|859.0696787| 1072.438262| 916.904998|1194.0707205|1262.4566786| 1300.499388|667.1068062|740.6703692|\n",
      "|2023-11-01 02:00:00|100.8072769|5.088635| 37.074778|70.51349|271.61032|39.99705|54.39896|  18.634|84.5498587| 789.551986|81.7818398|103.1938648|119.6217611|85.4490855|113.5615968|92.586014|116.4478637|425.3567916|0.9499696|259.4904194| 5.2302|89.7790462|73.7836437| 52.000126|28.8050612|   94.73286|125.9055434|10.93568|137.6821459|68.092325|42.2372844|62.8936218|170.2794481| 8.68515|67.5309245|11.3977542| 105.706871| 27.997667|60.599286|63.7675849|857.3136828|1090.5412446|921.0812677|1203.6258183|1277.8601186|1318.1026424|663.6489315|742.2439331|\n",
      "|2023-11-01 03:00:00| 89.3625387|5.091995|36.9181675|70.87265|270.43028|47.27289|54.90864|18.67764|84.4858804|782.4309257|84.7389786|103.9782605|109.3510624|86.4068319|113.9055097|94.938381|113.3230231|429.0882471|0.9518401|260.3734318|5.25192|91.3354338|74.6767012|52.1626682|32.8001032| 98.9616308| 125.952706| 10.1243|137.5993198|67.281979|43.1809826|63.8644234|171.1266798|8.514331|68.4336313|11.4582428| 108.017745| 27.576125|60.991115|66.5324379|878.4552624| 1097.247125|944.9877003|1184.0447819|1291.1471368|1315.4974333|673.3971215|749.9778629|\n",
      "|2023-11-01 04:00:00| 93.7056649|5.140805|36.0716814|71.12218|273.85204|56.72919|54.82088|18.51712|79.0731592|792.3616956|87.9610386|105.4031992|115.9632527|86.6481517|111.0687621|97.216264|112.5419782|430.7831459| 0.954124|266.5346496|  4.509|94.0897887|74.9168042|53.7390838|34.4198478|104.9060381|125.8053753| 9.51206|135.9694105|68.992275|46.2632821|65.1804212|171.7989118|8.862362|69.0972602|11.0212758| 112.954308|27.8952271|61.896075|70.4308881|941.2657119|1144.7112855|  1011.6966|1202.3727102|1346.8335876|1328.5480407|683.2880942|761.9666415|\n",
      "+-------------------+-----------+--------+----------+--------+---------+--------+--------+--------+----------+-----------+----------+-----------+-----------+----------+-----------+---------+-----------+-----------+---------+-----------+-------+----------+----------+----------+----------+-----------+-----------+--------+-----------+---------+----------+----------+-----------+--------+----------+----------+-----------+----------+---------+----------+-----------+------------+-----------+------------+------------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# First Step will be to Read Excel via pandas, then convert to Spark DF\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"CWD:\", os.getcwd())  # just to confirm we're in .../612FinalProjectGroup3/notebooks\n",
    "\n",
    "aeso_pd = pd.read_excel(\n",
    "    \"../data/LoadDataAesoRaw/Hourly-load-by-area-and-region-Nov-2023-to-Dec-2024.xlsx\",\n",
    "    engine=\"openpyxl\"\n",
    ")\n",
    "\n",
    "aeso_df = spark.createDataFrame(aeso_pd)\n",
    "\n",
    "aeso_df.printSchema()\n",
    "aeso_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa311662-1df0-4748-a9dc-86220c79a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AREA13',\n",
       " 'AREA17',\n",
       " 'AREA18',\n",
       " 'AREA19',\n",
       " 'AREA20',\n",
       " 'AREA21',\n",
       " 'AREA22',\n",
       " 'AREA23',\n",
       " 'AREA24',\n",
       " 'AREA25',\n",
       " 'AREA26',\n",
       " 'AREA27',\n",
       " 'AREA28',\n",
       " 'AREA29',\n",
       " 'AREA30',\n",
       " 'AREA31',\n",
       " 'AREA32',\n",
       " 'AREA33',\n",
       " 'AREA34',\n",
       " 'AREA35',\n",
       " 'AREA36',\n",
       " 'AREA37',\n",
       " 'AREA38',\n",
       " 'AREA39',\n",
       " 'AREA4',\n",
       " 'AREA40',\n",
       " 'AREA42',\n",
       " 'AREA43',\n",
       " 'AREA44',\n",
       " 'AREA45',\n",
       " 'AREA46',\n",
       " 'AREA47',\n",
       " 'AREA48',\n",
       " 'AREA49',\n",
       " 'AREA52',\n",
       " 'AREA53',\n",
       " 'AREA54',\n",
       " 'AREA55',\n",
       " 'AREA56',\n",
       " 'AREA57',\n",
       " 'AREA6',\n",
       " 'AREA60']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, regexp_extract\n",
    "\n",
    "# This is the actuall extraction of our area columns, so we can use them effectively\n",
    "area_cols = [c for c in aeso_df.columns if c.startswith(\"AREA\")]\n",
    "#showing them out to ensure they match.\n",
    "area_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "168f5a0b-fe6d-4798-87a4-ef36d821ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack(42, 'AREA13', AREA13, 'AREA17', AREA17, 'AREA18', AREA18, 'AREA19', AREA19, 'AREA20', AREA20, 'AREA21', AREA21, 'AREA22', AREA22, 'AREA23', AREA23, 'AREA24', AREA24, 'AREA25', AREA25, 'AREA26', AREA26, 'AREA27', AREA27, 'AREA28', AREA28, 'AREA29', AREA29, 'AREA30', AREA30, 'AREA31', AREA31, 'AREA32', AREA32, 'AREA33', AREA33, 'AREA34', AREA34, 'AREA35', AREA35, 'AREA36', AREA36, 'AREA37', AREA37, 'AREA38', AREA38, 'AREA39', AREA39, 'AREA4', AREA4, 'AREA40', AREA40, 'AREA42', AREA42, 'AREA43', AREA43, 'AREA44', AREA44, 'AREA45', AREA45, 'AREA46', AREA46, 'AREA47', AREA47, 'AREA48', AREA48, 'AREA49', AREA49, 'AREA52', AREA52, 'AREA53', AREA53, 'AREA54', AREA54, 'AREA55', AREA55, 'AREA56', AREA56, 'AREA57', AREA57, 'AREA6', AREA6, 'AREA60', AREA60) as (area_name, load_mw)\n"
     ]
    }
   ],
   "source": [
    "# 2 Making a stack of all the areas, which we will use fror getting all these names\n",
    "# will make our life much easier for the actual working\n",
    "\n",
    "stack_pairs = \", \".join([f\"'{c}', {c}\" for c in area_cols])\n",
    "stack_expr = f\"stack({len(area_cols)}, {stack_pairs}) as (area_name, load_mw)\"\n",
    "\n",
    "print(stack_expr)  #This is to ensure it matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7f9699a-2bda-4068-867e-0a96c40ff854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- area_name: string (nullable = true)\n",
      " |-- load_mw: double (nullable = true)\n",
      "\n",
      "+-------------------+---------+----------+\n",
      "|          timestamp|area_name|   load_mw|\n",
      "+-------------------+---------+----------+\n",
      "|2023-11-01 00:00:00|   AREA13|100.173998|\n",
      "|2023-11-01 00:00:00|   AREA17|  5.166425|\n",
      "|2023-11-01 00:00:00|   AREA18|37.6438593|\n",
      "|2023-11-01 00:00:00|   AREA19|  71.10166|\n",
      "|2023-11-01 00:00:00|   AREA20| 275.33804|\n",
      "+-------------------+---------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#This is no taking the timestamp and the stakc that we made above to format our table with the timestamp\n",
    "#eseentially we should have each and every area for each time stamp\n",
    "aeso_long = (\n",
    "    aeso_df\n",
    "    .selectExpr(\"DT_MST as timestamp\", stack_expr)\n",
    "    .where(\"load_mw is not null\")        # drop empty cells if any\n",
    ")\n",
    "\n",
    "aeso_long.printSchema()\n",
    "aeso_long.show(5)\n",
    "\n",
    "#okay whicked timestamp is a timepsamp format, but I see now everything is aggregated at the first time stamp\n",
    "#essentially sorted that way, which is good (didnt knwo for sure that was going to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f370f63b-de2d-4f52-bc99-e5e13f235cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+-----------+\n",
      "|          timestamp|area_name|area_code|    load_mw|\n",
      "+-------------------+---------+---------+-----------+\n",
      "|2023-11-01 00:00:00|   AREA13|       13| 100.173998|\n",
      "|2023-11-01 00:00:00|   AREA17|       17|   5.166425|\n",
      "|2023-11-01 00:00:00|   AREA18|       18| 37.6438593|\n",
      "|2023-11-01 00:00:00|   AREA19|       19|   71.10166|\n",
      "|2023-11-01 00:00:00|   AREA20|       20|  275.33804|\n",
      "|2023-11-01 00:00:00|   AREA21|       21|   45.18537|\n",
      "|2023-11-01 00:00:00|   AREA22|       22|   57.69576|\n",
      "|2023-11-01 00:00:00|   AREA23|       23|   18.43956|\n",
      "|2023-11-01 00:00:00|   AREA24|       24| 85.4734143|\n",
      "|2023-11-01 00:00:00|   AREA25|       25|786.5661492|\n",
      "+-------------------+---------+---------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "#Now uisng regexp_extract to take a part the actual numeric part of the area_name from our AESO DATA\n",
    "#this is huge as it prevents hard coding, always me to demonstarte knowledge of regexp and then we have this as just numerical\n",
    "#which will serve as the key for our furture join with our WEATHER Data\n",
    "\n",
    "aeso_long = (\n",
    "    aeso_long\n",
    "    .withColumn(\"area_code\",\n",
    "                regexp_extract(\"area_name\", r\"AREA(\\d+)\", 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "#selecting the specific columns of interest from AESO at this point to show (just the added area_code)\n",
    "aeso_long.select(\"timestamp\", \"area_name\", \"area_code\", \"load_mw\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5435429-19ca-4689-974f-82f8bf1705d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_code: long (nullable = true)\n",
      " |-- region_type: string (nullable = true)\n",
      " |-- location_name: string (nullable = true)\n",
      "\n",
      "+---------+-----------+--------------------+\n",
      "|area_code|region_type|       location_name|\n",
      "+---------+-----------+--------------------+\n",
      "|        4|      rural|        Medicine Hat|\n",
      "|        6|      urban|             Calgary|\n",
      "|       13|      rural|        Lloydminster|\n",
      "|       17|      rural|        Rainbow Lake|\n",
      "|       18|      rural|          High Level|\n",
      "|       19|      rural|         Peace River|\n",
      "|       20|      rural|      Grande Prairie|\n",
      "|       21|      rural|        High Prairie|\n",
      "|       22|      rural|        Grande Cache|\n",
      "|       23|      rural|          Valleyview|\n",
      "|       24|      rural|           Fox Creek|\n",
      "|       25|      rural|       Fort McMurray|\n",
      "|       26|      rural|          Swan Hills|\n",
      "|       27|      rural|Athabasca/Lac La ...|\n",
      "|       28|      rural|           Cold Lake|\n",
      "|       29|      rural|        Hinton/Edson|\n",
      "|       30|      rural|      Drayton Valley|\n",
      "|       31|      urban|          Wetaskiwin|\n",
      "|       32|      rural|          Wainwright|\n",
      "|       33|      urban|   Fort Saskatchewan|\n",
      "|       34|      rural|        Abraham Lake|\n",
      "|       35|      rural|            Red Deer|\n",
      "|       36|      rural|Alliance/Battle R...|\n",
      "|       37|      rural|             Provost|\n",
      "|       38|      rural|            Caroline|\n",
      "|       39|      rural|            Didsbury|\n",
      "|       40|      urban|             Wabamun|\n",
      "|       42|      rural|               Hanna|\n",
      "|       43|      rural|           Sheerness|\n",
      "|       44|      urban|               Seebe|\n",
      "|       45|      urban|  Strathmore/Blackie|\n",
      "|       46|      urban|          High River|\n",
      "|       47|      rural|              Brooks|\n",
      "|       48|      rural|             Empress|\n",
      "|       49|      rural|             Stavely|\n",
      "|       52|      urban|            Vauxhall|\n",
      "|       53|      urban|        Fort Macleod|\n",
      "|       54|      urban|          Lethbridge|\n",
      "|       55|      urban|            Glenwood|\n",
      "|       56|      urban|          Vegreville|\n",
      "|       57|      urban|             Airdrie|\n",
      "|       60|      urban|            Edmonton|\n",
      "+---------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#now it's time to load of our custom csv which indidcates each region as urban or rural \n",
    "#our reasoning and justification can be found in our report\n",
    "#going to also join this based on area code\n",
    "mapping_pd = pd.read_csv(\"../data/area_region_mapping.csv\")\n",
    "mapping_df = spark.createDataFrame(mapping_pd)\n",
    "\n",
    "mapping_df.printSchema()\n",
    "mapping_df.show(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dbb5265-ef75-48d3-b454-f6a0ca5b356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 580:>                                                      (0 + 24) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+-----------+-------------+\n",
      "|area_code|          timestamp|area_name| load_mw|region_type|location_name|\n",
      "+---------+-------------------+---------+--------+-----------+-------------+\n",
      "|       19|2023-11-01 00:00:00|   AREA19|71.10166|      rural|  Peace River|\n",
      "|       19|2023-11-01 01:00:00|   AREA19| 71.0077|      rural|  Peace River|\n",
      "|       19|2023-11-01 02:00:00|   AREA19|70.51349|      rural|  Peace River|\n",
      "|       19|2023-11-01 03:00:00|   AREA19|70.87265|      rural|  Peace River|\n",
      "|       19|2023-11-01 04:00:00|   AREA19|71.12218|      rural|  Peace River|\n",
      "|       19|2023-11-01 05:00:00|   AREA19|73.80247|      rural|  Peace River|\n",
      "|       19|2023-11-01 06:00:00|   AREA19| 79.6731|      rural|  Peace River|\n",
      "|       19|2023-11-01 07:00:00|   AREA19|83.85579|      rural|  Peace River|\n",
      "|       19|2023-11-01 08:00:00|   AREA19|83.11333|      rural|  Peace River|\n",
      "|       19|2023-11-01 09:00:00|   AREA19|82.57626|      rural|  Peace River|\n",
      "|       19|2023-11-01 10:00:00|   AREA19|82.54175|      rural|  Peace River|\n",
      "|       19|2023-11-01 11:00:00|   AREA19|82.32896|      rural|  Peace River|\n",
      "|       19|2023-11-01 12:00:00|   AREA19|82.33383|      rural|  Peace River|\n",
      "|       19|2023-11-01 13:00:00|   AREA19|81.63458|      rural|  Peace River|\n",
      "|       19|2023-11-01 14:00:00|   AREA19| 80.9923|      rural|  Peace River|\n",
      "|       19|2023-11-01 15:00:00|   AREA19|81.99139|      rural|  Peace River|\n",
      "|       19|2023-11-01 16:00:00|   AREA19| 81.0918|      rural|  Peace River|\n",
      "|       19|2023-11-01 17:00:00|   AREA19|81.12173|      rural|  Peace River|\n",
      "|       19|2023-11-01 18:00:00|   AREA19|80.10084|      rural|  Peace River|\n",
      "|       19|2023-11-01 19:00:00|   AREA19|79.11794|      rural|  Peace River|\n",
      "|       19|2023-11-01 20:00:00|   AREA19|78.02195|      rural|  Peace River|\n",
      "|       19|2023-11-01 21:00:00|   AREA19|75.59988|      rural|  Peace River|\n",
      "|       19|2023-11-01 22:00:00|   AREA19|72.68034|      rural|  Peace River|\n",
      "|       19|2023-11-01 23:00:00|   AREA19|72.17148|      rural|  Peace River|\n",
      "|       19|2023-11-02 00:00:00|   AREA19| 70.3471|      rural|  Peace River|\n",
      "|       19|2023-11-02 01:00:00|   AREA19| 68.9646|      rural|  Peace River|\n",
      "|       19|2023-11-02 02:00:00|   AREA19|69.18738|      rural|  Peace River|\n",
      "|       19|2023-11-02 03:00:00|   AREA19|68.76806|      rural|  Peace River|\n",
      "|       19|2023-11-02 04:00:00|   AREA19|67.84752|      rural|  Peace River|\n",
      "|       19|2023-11-02 05:00:00|   AREA19|70.66142|      rural|  Peace River|\n",
      "|       19|2023-11-02 06:00:00|   AREA19|78.03804|      rural|  Peace River|\n",
      "|       19|2023-11-02 07:00:00|   AREA19| 82.0169|      rural|  Peace River|\n",
      "|       19|2023-11-02 08:00:00|   AREA19|  84.197|      rural|  Peace River|\n",
      "|       19|2023-11-02 09:00:00|   AREA19|83.44983|      rural|  Peace River|\n",
      "|       19|2023-11-02 10:00:00|   AREA19|82.49199|      rural|  Peace River|\n",
      "|       19|2023-11-02 11:00:00|   AREA19|82.88735|      rural|  Peace River|\n",
      "|       19|2023-11-02 12:00:00|   AREA19|83.08252|      rural|  Peace River|\n",
      "|       19|2023-11-02 13:00:00|   AREA19| 83.0808|      rural|  Peace River|\n",
      "|       19|2023-11-02 14:00:00|   AREA19|83.10995|      rural|  Peace River|\n",
      "|       19|2023-11-02 15:00:00|   AREA19| 84.6063|      rural|  Peace River|\n",
      "|       19|2023-11-02 16:00:00|   AREA19|82.41299|      rural|  Peace River|\n",
      "|       19|2023-11-02 17:00:00|   AREA19|83.42839|      rural|  Peace River|\n",
      "|       19|2023-11-02 18:00:00|   AREA19|83.47526|      rural|  Peace River|\n",
      "|       19|2023-11-02 19:00:00|   AREA19|82.00496|      rural|  Peace River|\n",
      "|       19|2023-11-02 20:00:00|   AREA19|80.78856|      rural|  Peace River|\n",
      "|       19|2023-11-02 21:00:00|   AREA19|77.92011|      rural|  Peace River|\n",
      "|       19|2023-11-02 22:00:00|   AREA19|75.17319|      rural|  Peace River|\n",
      "|       19|2023-11-02 23:00:00|   AREA19|73.52665|      rural|  Peace River|\n",
      "|       19|2023-11-03 00:00:00|   AREA19|73.14961|      rural|  Peace River|\n",
      "|       19|2023-11-03 01:00:00|   AREA19|71.56074|      rural|  Peace River|\n",
      "+---------+-------------------+---------+--------+-----------+-------------+\n",
      "only showing top 50 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Now time for our first merge, before weather we are joining the region to each of our data points\n",
    "#based on the area code -> we did this lots in 612, so this was straightforward with the join\n",
    "#on and of course inner -> beauty of pyspark and the allowing of pseudo SQL commands\n",
    "\n",
    "aeso_with_region = (\n",
    "    aeso_long\n",
    "    .join(mapping_df, on=\"area_code\", how=\"inner\")\n",
    ")\n",
    "\n",
    "aeso_with_region.select(\n",
    "    \"timestamp\", \"area_code\", \"area_name\",\n",
    "    \"location_name\", \"region_type\", \"load_mw\"\n",
    ")\n",
    "\n",
    "aeso_with_region.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936c4cc-9937-4fbf-8b62-b5fdb85f7dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d3ac43c-1281-406c-98e3-58792a8d1e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 585:======>                                                (3 + 21) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------------+\n",
      "|          timestamp|region_type|     total_load_mw|\n",
      "+-------------------+-----------+------------------+\n",
      "|2023-11-01 13:00:00|      rural|       3091.182386|\n",
      "|2023-11-05 22:00:00|      rural|      3210.6321512|\n",
      "|2023-11-12 00:00:00|      urban|3290.9197912000004|\n",
      "|2023-11-26 20:00:00|      urban|4010.1097236000005|\n",
      "|2023-12-03 23:00:00|      urban|      3482.9017276|\n",
      "+-------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- region_type: string (nullable = true)\n",
      " |-- total_load_mw: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as F_sum\n",
    "\n",
    "#Grouping by region type, thought this would be cool to look at so essentially just reshows each timeperiod as urban or rural instead of area,\n",
    "# perhaps not super valuable but not bad.\n",
    "urban_rural_long = (\n",
    "    aeso_with_region\n",
    "    .groupBy(\"timestamp\", \"region_type\")\n",
    "    .agg(F_sum(\"load_mw\").alias(\"total_load_mw\"))\n",
    ")\n",
    "\n",
    "urban_rural_long.show(5)\n",
    "urban_rural_long.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fde7e49e-c2d9-4c73-8f64-07575979085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 594:====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|          timestamp|             urban|             rural|\n",
      "+-------------------+------------------+------------------+\n",
      "|2023-11-01 00:00:00|      3201.0208318|2992.1147807999996|\n",
      "|2023-11-01 01:00:00|3126.1436237999997|2955.5653366999995|\n",
      "|2023-11-01 02:00:00|      3145.5424359|      2981.0202757|\n",
      "|2023-11-01 03:00:00|3189.7622247000004|2969.2898119999995|\n",
      "|2023-11-01 04:00:00|      3323.4024472|3011.3032270000003|\n",
      "|2023-11-01 05:00:00|3669.7011083999996|      3075.4196297|\n",
      "|2023-11-01 06:00:00|3983.8241967000004|      3167.2820205|\n",
      "|2023-11-01 07:00:00|      4128.9717709|      3149.9376985|\n",
      "|2023-11-01 08:00:00|      4116.7720527|      3148.5074315|\n",
      "|2023-11-01 09:00:00|4057.8694981999997|      3141.5562506|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- urban: double (nullable = true)\n",
      " |-- rural: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "\n",
    "#Grouping by region type, thought this would be cool to look at so essentially combining every area for each time\n",
    "#into urban and rural, not sure if we will use this past this but not bad\n",
    "\n",
    "urban_rural = (\n",
    "    urban_rural_long\n",
    "    .groupBy(\"timestamp\")\n",
    "    .pivot(\"region_type\", [\"urban\", \"rural\"])  # order the columns explicitly\n",
    "    .agg(first(\"total_load_mw\"))\n",
    "    .orderBy(\"timestamp\")\n",
    ")\n",
    "\n",
    "urban_rural.show(10)\n",
    "urban_rural.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86637e27-97e2-4c86-be01-8958b337ad29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "204122ac-63da-40c8-a74e-e6ba5a097189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 608:>                                                      (0 + 24) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+----+---------+-----+\n",
      "|          timestamp|             urban|             rural|hour|dayofweek|month|\n",
      "+-------------------+------------------+------------------+----+---------+-----+\n",
      "|2023-11-01 00:00:00|      3201.0208318|2992.1147807999996|   0|        4|   11|\n",
      "|2023-11-01 01:00:00|3126.1436237999997|2955.5653366999995|   1|        4|   11|\n",
      "|2023-11-01 02:00:00|      3145.5424359|      2981.0202757|   2|        4|   11|\n",
      "|2023-11-01 03:00:00|3189.7622247000004|2969.2898119999995|   3|        4|   11|\n",
      "|2023-11-01 04:00:00|      3323.4024472|3011.3032270000003|   4|        4|   11|\n",
      "|2023-11-01 05:00:00|3669.7011083999996|      3075.4196297|   5|        4|   11|\n",
      "|2023-11-01 06:00:00|3983.8241967000004|      3167.2820205|   6|        4|   11|\n",
      "|2023-11-01 07:00:00|      4128.9717709|      3149.9376985|   7|        4|   11|\n",
      "|2023-11-01 08:00:00|      4116.7720527|      3148.5074315|   8|        4|   11|\n",
      "|2023-11-01 09:00:00|4057.8694981999997|      3141.5562506|   9|        4|   11|\n",
      "+-------------------+------------------+------------------+----+---------+-----+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month\n",
    "\n",
    "#okay this where we are actually adding features, I knew I wanted to achieve this and with some help from CHATGPT\n",
    "#using the import of hour day of the week and month this turned out beautiful, these are categorical featurs\n",
    "#in my eyes and allows for encoding and vectorizing features to help our model!!!!!\n",
    "#as day of the week is super important, I think esepcially in the scope of Energy usage right, like wweekend vs holiday, so the fact\n",
    "#that we have this is great, and same for month and lastly hour -> helps understand if people are home or not ect, so these\n",
    "#are a bunch of of great features to add to our data set.\n",
    "\n",
    "#doing this on our tester urban_rural_features -> this is not our main dataframe which will do next\n",
    "urban_rural_features = (\n",
    "    urban_rural\n",
    "    .withColumn(\"hour\", hour(\"timestamp\"))\n",
    "    .withColumn(\"dayofweek\", dayofweek(\"timestamp\"))  # This is directly from pypark.sql.functions, upon doing more into this librayr\n",
    "        #Sunday Maps to 1 and Saturday maps to 7, so this confroms that November 1st 2023 was a Wednesday.\n",
    "    .withColumn(\"month\", month(\"timestamp\"))\n",
    ")\n",
    "\n",
    "urban_rural_features.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c69967-56c0-4197-95f7-4bdf40fee459",
   "metadata": {},
   "source": [
    "Okay this all working beautiful as you can see above  \n",
    "\n",
    "This is exactly what we wanted as we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed72963-0e3c-409c-a659-a5e57a4e986b",
   "metadata": {},
   "source": [
    "Lastly we want to add Day of The Week, Month & Season. To Aeso with region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b24d617-4e4b-4d28-9ab5-dcfc013f9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5600:======>                                               (3 + 21) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------------+-----------+----+---------+-----+------+--------+\n",
      "|          timestamp|area_code|location_name|region_type|hour|dayofweek|month|season| load_mw|\n",
      "+-------------------+---------+-------------+-----------+----+---------+-----+------+--------+\n",
      "|2023-11-01 00:00:00|       19|  Peace River|      rural|   0|        4|   11|  fall|71.10166|\n",
      "|2023-11-01 01:00:00|       19|  Peace River|      rural|   1|        4|   11|  fall| 71.0077|\n",
      "|2023-11-01 02:00:00|       19|  Peace River|      rural|   2|        4|   11|  fall|70.51349|\n",
      "|2023-11-01 03:00:00|       19|  Peace River|      rural|   3|        4|   11|  fall|70.87265|\n",
      "|2023-11-01 04:00:00|       19|  Peace River|      rural|   4|        4|   11|  fall|71.12218|\n",
      "|2023-11-01 05:00:00|       19|  Peace River|      rural|   5|        4|   11|  fall|73.80247|\n",
      "|2023-11-01 06:00:00|       19|  Peace River|      rural|   6|        4|   11|  fall| 79.6731|\n",
      "|2023-11-01 07:00:00|       19|  Peace River|      rural|   7|        4|   11|  fall|83.85579|\n",
      "|2023-11-01 08:00:00|       19|  Peace River|      rural|   8|        4|   11|  fall|83.11333|\n",
      "|2023-11-01 09:00:00|       19|  Peace River|      rural|   9|        4|   11|  fall|82.57626|\n",
      "+-------------------+---------+-------------+-----------+----+---------+-----+------+--------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month, when, col\n",
    "\n",
    "#Now we want to what we just did but on our working Dataframe which is in the form we wanted\n",
    "#first will add hour, month day of week, but also SEASON!! this is another cool add that I wanted to see and\n",
    "#could be of importance.\n",
    "aeso_time_features = (\n",
    "    aeso_with_region\n",
    "        # 1) Hour of day (0â€“23)\n",
    "        .withColumn(\"hour\", hour(\"timestamp\"))\n",
    "\n",
    "        # 2) Day of week (Spark: 1 = Sunday, ..., 7 = Saturday)\n",
    "        .withColumn(\"dayofweek\", dayofweek(\"timestamp\"))\n",
    "\n",
    "        # 3) Month number (1 = January, ..., 12 = December)\n",
    "        .withColumn(\"month\", month(\"timestamp\"))\n",
    "\n",
    "        # 4) Season (meteorological seasons for Alberta / N. hemisphere)\n",
    "        .withColumn(\n",
    "            \"season\",\n",
    "            when(col(\"month\").isin(12, 1, 2),  \"winter\")\n",
    "            .when(col(\"month\").isin(3, 4, 5),  \"spring\")\n",
    "            .when(col(\"month\").isin(6, 7, 8),  \"summer\")\n",
    "            .otherwise(\"fall\")  # 9,10,11\n",
    "        )\n",
    ")\n",
    "\n",
    "#now we can see all our features together and looking not bad at all, this is quite good for our AESO data\n",
    "#we will start here for our models, and then see if we can get creative and add more features.\n",
    "aeso_time_features.select(\n",
    "    \"timestamp\", \"area_code\", \"location_name\", \"region_type\",\n",
    "    \"hour\", \"dayofweek\", \"month\", \"season\", \"load_mw\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2bb9f-70e6-407d-8449-05cb413456f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78218a21-8a27-469a-b85d-0dc69acc89e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "113c0453-5082-4e15-9386-d9adbeeeff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_code: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- temp_c: string (nullable = true)\n",
      " |-- precip_mm: string (nullable = true)\n",
      " |-- daylight: string (nullable = true)\n",
      "\n",
      "+---------+-------------------+------+---------+--------+\n",
      "|area_code|          timestamp|temp_c|precip_mm|daylight|\n",
      "+---------+-------------------+------+---------+--------+\n",
      "|        4|2023-11-01 00:00:00|  -4.6|      0.0|       0|\n",
      "|        4|2023-11-01 01:00:00|  -5.9|      0.0|       0|\n",
      "|        4|2023-11-01 02:00:00|  -6.5|      0.0|       0|\n",
      "|        4|2023-11-01 03:00:00|  -6.9|      0.0|       0|\n",
      "|        4|2023-11-01 04:00:00|  -6.8|      0.0|       0|\n",
      "+---------+-------------------+------+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#Okay now that our first script we made our full weather data frame and wrote it to a final CSV\n",
    "# we can make a spark for it, so we have and can join with our spark data frame of AESO\n",
    "\n",
    "#and we will be in great shape to build our first model\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"612 Final Project - Combine AESO & Weather\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "weather_spark = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .csv(\"../data/weather_all_areas_hourly_with_daylight.csv\")\n",
    ")\n",
    "\n",
    "weather_spark.printSchema()\n",
    "weather_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec1461ae-ba2d-49f7-b0ee-c5d39f87cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_code: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- temp_c: double (nullable = true)\n",
      " |-- precip_mm: double (nullable = true)\n",
      " |-- daylight: integer (nullable = true)\n",
      "\n",
      "+---------+-------------------+------+---------+--------+\n",
      "|area_code|          timestamp|temp_c|precip_mm|daylight|\n",
      "+---------+-------------------+------+---------+--------+\n",
      "|        4|2023-11-01 00:00:00|  -4.6|      0.0|       0|\n",
      "|        4|2023-11-01 01:00:00|  -5.9|      0.0|       0|\n",
      "|        4|2023-11-01 02:00:00|  -6.5|      0.0|       0|\n",
      "|        4|2023-11-01 03:00:00|  -6.9|      0.0|       0|\n",
      "|        4|2023-11-01 04:00:00|  -6.8|      0.0|       0|\n",
      "+---------+-------------------+------+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#realized from our schema above that our TIMESTAMP was a string not a TIMESTAMP, so renamed above BAD,\n",
    "#now we do casting to ensure our schema is perfect\n",
    "#this will be critical for our joining as we are doing by both TIMESTAMPE and of Course Area_Code\n",
    "\n",
    "weather_spark_cleaned = (\n",
    "    weather_spark\n",
    "        .withColumn(\"area_code\",  col(\"area_code\").cast(\"int\"))\n",
    "        .withColumn(\"timestamp\",  to_timestamp(\"timestamp\"))\n",
    "        .withColumn(\"temp_c\",     col(\"temp_c\").cast(\"double\"))\n",
    "        .withColumn(\"precip_mm\",  col(\"precip_mm\").cast(\"double\"))\n",
    "        .withColumn(\"daylight\",  col(\"daylight\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "weather_spark_cleaned.printSchema()\n",
    "weather_spark_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e7d34448-6c2d-4b8f-87d0-2bcc4ec695ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- area_code: integer (nullable = true)\n",
      " |-- area_name: string (nullable = true)\n",
      " |-- load_mw: double (nullable = true)\n",
      " |-- region_type: string (nullable = true)\n",
      " |-- location_name: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- temp_c: string (nullable = true)\n",
      " |-- precip_mm: string (nullable = true)\n",
      " |-- daylight: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5608:====>           (1 + 3) / 4][Stage 5609:=======>     (13 + 11) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+------------+-----------+-----------------+----+---------+-----+------+------+---------+--------+\n",
      "|          timestamp|area_code|area_name|     load_mw|region_type|    location_name|hour|dayofweek|month|season|temp_c|precip_mm|daylight|\n",
      "+-------------------+---------+---------+------------+-----------+-----------------+----+---------+-----+------+------+---------+--------+\n",
      "|2023-11-01 00:00:00|       29|   AREA29|  84.1644369|      rural|     Hinton/Edson|   0|        4|   11|  fall|  -2.8|      0.0|       0|\n",
      "|2023-11-01 00:00:00|       44|   AREA44| 137.9625539|      urban|            Seebe|   0|        4|   11|  fall|  -1.2|      0.0|       0|\n",
      "|2023-11-01 00:00:00|       49|   AREA49|    9.393875|      rural|          Stavely|   0|        4|   11|  fall|  -5.1|      0.0|       0|\n",
      "|2023-11-01 00:00:00|       60|   AREA60|1123.2788268|      urban|         Edmonton|   0|        4|   11|  fall|   0.8|      0.0|       0|\n",
      "|2023-11-01 02:00:00|        4|    AREA4|  28.8050612|      rural|     Medicine Hat|   2|        4|   11|  fall|  -6.5|      0.0|       0|\n",
      "|2023-11-01 02:00:00|       20|   AREA20|   271.61032|      rural|   Grande Prairie|   2|        4|   11|  fall|  -2.0|     NULL|       0|\n",
      "|2023-11-01 02:00:00|       53|   AREA53|  11.3977542|      urban|     Fort Macleod|   2|        4|   11|  fall|  -7.4|      0.0|       0|\n",
      "|2023-11-01 03:00:00|       33|   AREA33| 429.0882471|      urban|Fort Saskatchewan|   3|        4|   11|  fall|  -2.4|      0.0|       0|\n",
      "|2023-11-01 03:00:00|       46|   AREA46|  43.1809826|      urban|       High River|   3|        4|   11|  fall|  -8.9|      0.0|       0|\n",
      "|2023-11-01 03:00:00|       55|   AREA55|   27.576125|      urban|         Glenwood|   3|        4|   11|  fall|  -5.5|      0.0|       0|\n",
      "+-------------------+---------+---------+------------+-----------+-----------------+----+---------+-----+------+------+---------+--------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#This is the crux, which the join, which will we do on each timestampe(now that they match) and the area code,\n",
    "# so after this another column for both precipiation and temprature will be added for each row respective to it\n",
    "\n",
    "#took a ton of coordination, preparation and manipulation, but it looks great!! #exactly as intended\n",
    "\n",
    "aeso_weather = (\n",
    "    aeso_time_features\n",
    "        .join(\n",
    "            weather_spark,\n",
    "            on=[\"timestamp\", \"area_code\"],  \n",
    "            how=\"inner\"                     \n",
    "        )\n",
    ")\n",
    "\n",
    "aeso_weather.printSchema()\n",
    "aeso_weather.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2543987-b224-46ff-aa14-9f5badae3141",
   "metadata": {},
   "source": [
    "# FIXING SCHEMA\n",
    "\n",
    "Good thing I checked the schema, because our temp_c and precip_m are STRINGS\n",
    "That was a big miss, dint even realize until vector assembly and got an error as you can only take in numeric or vector columns\n",
    "WHole reason for encoding, so I will just type cast them as doubles here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7bcaf3de-2d50-4c12-8342-6aac774a1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- area_code: integer (nullable = true)\n",
      " |-- area_name: string (nullable = true)\n",
      " |-- load_mw: double (nullable = true)\n",
      " |-- region_type: string (nullable = true)\n",
      " |-- location_name: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- temp_c: double (nullable = true)\n",
      " |-- precip_mm: double (nullable = true)\n",
      " |-- daylight: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aeso_weather = (\n",
    "    aeso_weather\n",
    "        .withColumn(\"temp_c\", col(\"temp_c\").cast(\"double\"))\n",
    "        .withColumn(\"precip_mm\", col(\"precip_mm\").cast(\"double\"))\n",
    "        .withColumn(\"daylight\", col(\"daylight\").cast(\"int\"))  # ADDED: Ensure daylight is int\n",
    ")\n",
    "\n",
    "aeso_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12e0597e-71cc-453d-983c-c807f427ca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- area_code: integer (nullable = true)\n",
      " |-- area_name: string (nullable = true)\n",
      " |-- load_mw: double (nullable = true)\n",
      " |-- region_type: string (nullable = true)\n",
      " |-- location_name: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- temp_c: double (nullable = true)\n",
      " |-- precip_mm: double (nullable = true)\n",
      " |-- daylight: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+------------+-----------+-----------------+----+---------+-----+------+------+---------+--------+\n",
      "|timestamp          |area_code|area_name|load_mw     |region_type|location_name    |hour|dayofweek|month|season|temp_c|precip_mm|daylight|\n",
      "+-------------------+---------+---------+------------+-----------+-----------------+----+---------+-----+------+------+---------+--------+\n",
      "|2023-11-01 00:00:00|29       |AREA29   |84.1644369  |rural      |Hinton/Edson     |0   |4        |11   |fall  |-2.8  |0.0      |0       |\n",
      "|2023-11-01 00:00:00|44       |AREA44   |137.9625539 |urban      |Seebe            |0   |4        |11   |fall  |-1.2  |0.0      |0       |\n",
      "|2023-11-01 00:00:00|49       |AREA49   |9.393875    |rural      |Stavely          |0   |4        |11   |fall  |-5.1  |0.0      |0       |\n",
      "|2023-11-01 00:00:00|60       |AREA60   |1123.2788268|urban      |Edmonton         |0   |4        |11   |fall  |0.8   |0.0      |0       |\n",
      "|2023-11-01 02:00:00|4        |AREA4    |28.8050612  |rural      |Medicine Hat     |2   |4        |11   |fall  |-6.5  |0.0      |0       |\n",
      "|2023-11-01 02:00:00|20       |AREA20   |271.61032   |rural      |Grande Prairie   |2   |4        |11   |fall  |-2.0  |NULL     |0       |\n",
      "|2023-11-01 02:00:00|53       |AREA53   |11.3977542  |urban      |Fort Macleod     |2   |4        |11   |fall  |-7.4  |0.0      |0       |\n",
      "|2023-11-01 03:00:00|33       |AREA33   |429.0882471 |urban      |Fort Saskatchewan|3   |4        |11   |fall  |-2.4  |0.0      |0       |\n",
      "|2023-11-01 03:00:00|46       |AREA46   |43.1809826  |urban      |High River       |3   |4        |11   |fall  |-8.9  |0.0      |0       |\n",
      "|2023-11-01 03:00:00|55       |AREA55   |27.576125   |urban      |Glenwood         |3   |4        |11   |fall  |-5.5  |0.0      |0       |\n",
      "+-------------------+---------+---------+------------+-----------+-----------------+----+---------+-----+------+------+---------+--------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5630:====>           (1 + 3) / 4][Stage 5631:=============>(23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+-------------------+\n",
      "|summary|          load_mw|            temp_c|         precip_mm|           daylight|\n",
      "+-------+-----------------+------------------+------------------+-------------------+\n",
      "|  count|           426840|            416592|            362015|             426840|\n",
      "|   mean|165.8880732508191|2.9598772900103727|0.0408850461997432|0.43811732733576986|\n",
      "| stddev|272.9519930960776|12.370707610516746|0.3452698979226287|  0.496156337814179|\n",
      "|    min|              0.0|             -47.4|               0.0|                  0|\n",
      "|    max|      2121.356533|              38.0|              44.4|                  1|\n",
      "+-------+-----------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aeso_weather.printSchema()\n",
    "aeso_weather.show(10, truncate=False)\n",
    "aeso_weather.describe([\"load_mw\", \"temp_c\", \"precip_mm\",\"daylight\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "229f3656-eccb-4b7b-a5b1-1da33d45dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5648:==========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------+--------------+---------------+---------------+\n",
      "|load_mw_nulls|temp_c_nulls|precip_mm_nulls|daylight_nulls|area_code_nulls|timestamp_nulls|\n",
      "+-------------+------------+---------------+--------------+---------------+---------------+\n",
      "|            0|       10248|          64825|             0|              0|              0|\n",
      "+-------------+------------+---------------+--------------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as F_sum\n",
    "\n",
    "aeso_weather.select([\n",
    "    F_sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulls\")\n",
    "    for c in [\"load_mw\", \"temp_c\", \"precip_mm\", \"daylight\", \"area_code\", \"timestamp\"]\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d76632-d5f3-4a15-a5c5-0214681758c4",
   "metadata": {},
   "source": [
    "# DEALING WITH NULLS FOR FINAL DATA SET\n",
    "\n",
    "So we realized we NULLS in both temp_c_nulls and precip_mm_nulls, so the before we can do any modelling we should deal with them.\n",
    "\n",
    "Of course in 612 Week 6 we learned a ton of techniques to deal with that, and will apply one as such.\n",
    "\n",
    "After deliberation I think we realized that for precipitation, that rain is more random than temprature, just because it rained another day that month isnt neccesarily indicative that is raining again, what we think is the best strategy as when it rains (it rains for generally a while), is to fill nulls with the average precipation of the whole (average of every hour) in that SPECIFIC REGION and replace the rain for the null hours with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "952b404c-f770-4d40-9baa-4dbe37eb8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "aeso_weather_day = (\n",
    "    aeso_weather\n",
    "        .withColumn(\"day\", to_date(\"timestamp\"))  # e.g. 2023-11-01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10c0b513-db7a-49be-8252-5139695a218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, when, col\n",
    "\n",
    "w_daily = Window.partitionBy(\"area_code\", \"day\")\n",
    "\n",
    "aeso_weather_precip_filled = (\n",
    "    aeso_weather_day\n",
    "        # average of non-null precip_mm for that region+day\n",
    "        .withColumn(\"daily_precip_avg\", avg(\"precip_mm\").over(w_daily))\n",
    "        # fill only where precip_mm is null\n",
    "        .withColumn(\n",
    "            \"precip_mm_filled\",\n",
    "            when(col(\"precip_mm\").isNull(), col(\"daily_precip_avg\"))\n",
    "            .otherwise(col(\"precip_mm\"))\n",
    "        )\n",
    "        .drop(\"daily_precip_avg\")  # optional cleanup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a11bd831-8027-4b2a-9621-0651afb46e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "aeso_weather_clean_precipation = (\n",
    "    aeso_weather_precip_filled\n",
    "        .drop(\"precip_mm\")\n",
    "        .withColumnRenamed(\"precip_mm_filled\", \"precip_mm\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "406eba0b-53ee-48dc-9edd-bbf295f417ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5665:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------+---------------+---------------+\n",
      "|load_mw_nulls|temp_c_nulls|precip_mm_nulls|area_code_nulls|timestamp_nulls|\n",
      "+-------------+------------+---------------+---------------+---------------+\n",
      "|            0|       10248|          64824|              0|              0|\n",
      "+-------------+------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Rime to check\n",
    "aeso_weather_clean_precipation.select([\n",
    "    F_sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulls\")\n",
    "    for c in [\"load_mw\", \"temp_c\", \"precip_mm\", \"area_code\", \"timestamp\"]\n",
    "]).show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c66fd-60a1-48db-ba1c-68f421fbc827",
   "metadata": {},
   "source": [
    "As that had almost no impact, what this now means is that most likely we just dont have data for a set period so there is nothing in that day to take the avergae from and we cant replace NULLS,  \n",
    "\n",
    "We need to inspect further so my next step was to see where the nulls actually live (in which areas and then I can do an intentional exploration on the data at hand.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8db5db75-db35-42a2-82e8-a434388e5933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|area_code|month|\n",
      "+---------+-----+\n",
      "|       19|    2|\n",
      "|       26|    4|\n",
      "|       26|   12|\n",
      "|       26|    1|\n",
      "|       19|    1|\n",
      "|       19|    4|\n",
      "|       22|   11|\n",
      "|       26|   11|\n",
      "|       19|    6|\n",
      "|       26|    3|\n",
      "|       22|   10|\n",
      "|       19|    7|\n",
      "|       26|   10|\n",
      "|       19|   12|\n",
      "|       19|    9|\n",
      "|       19|    3|\n",
      "|       19|    5|\n",
      "|       26|    2|\n",
      "|       19|    8|\n",
      "|       19|   10|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# look at distinct (area_code, month) where precip_c is null\n",
    "aeso_weather.filter(col(\"precip_mm\").isNull()) \\\n",
    "    .select(\"area_code\", \"month\") \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26f614-a185-4089-b5a6-16aa6ee424f9",
   "metadata": {},
   "source": [
    "# WHAT THIS CHECK TELLS US ABOUT NULLS\n",
    "This tells us it is not a bad weather station issue with corrupt data there are nulls across many different weather stations without a specific concentration in one, after further inspection into the raw data of the stations of interest (connected to the area code provided), this was confiemed\n",
    "\n",
    "Lets go back to our original data set and just fill them with 0, thats also probably pretty safe when it comes to rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb86755-f030-499e-8ed0-9558bec7f830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "909c01bc-7b6f-4174-a4ab-f50593aa5fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------+---------------+---------------+\n",
      "|load_mw_nulls|temp_c_nulls|precip_mm_nulls|area_code_nulls|timestamp_nulls|\n",
      "+-------------+------------+---------------+---------------+---------------+\n",
      "|            0|       10248|              0|              0|              0|\n",
      "+-------------+------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aeso_weather = aeso_weather.fillna({\n",
    "    \"precip_mm\": 0.0,\n",
    "    \"daylight\": 0  \n",
    "})\n",
    "\n",
    "#check\n",
    "aeso_weather.select([\n",
    "    F_sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulls\")\n",
    "    for c in [\"load_mw\", \"temp_c\", \"precip_mm\", \"area_code\", \"timestamp\"]\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be8382-3d73-4506-ae14-114ac631eae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704d4c04-a15e-4b4d-8b4b-0ee7d6d04cf9",
   "metadata": {},
   "source": [
    "# SUMMARY TIME OF NUMERICAL AND CATEGORICAL COLUMNS BEFORE MODELLING\n",
    "\n",
    "Just as we leaned (and taken from our work in Assignment 3), I learned its great to look first at both our Numerical and Categorical Columns (which of course will indexed and vectorized), before building our final pipeline to predict Load in MW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "58bba313-cb2f-4a5c-87b8-b1a108b8574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first of course we want to predict load for a certain timestamp, so we can extend that to the future (not including the year),\n",
    "\n",
    "label_col = \"load_mw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1eea1b64-5e90-48b1-a53a-fe104ff0b9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+-------------------+\n",
      "|summary|            temp_c|          precip_mm|           daylight|\n",
      "+-------+------------------+-------------------+-------------------+\n",
      "|  count|            416592|             426840|             426840|\n",
      "|   mean| 2.959877290010362|0.03467575672383007|0.43811732733576986|\n",
      "| stddev|12.370707610516764| 0.3183107378239392| 0.4961563378141771|\n",
      "|    min|             -47.4|                0.0|                  0|\n",
      "|    max|              38.0|               44.4|                  1|\n",
      "+-------+------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['temp_c', 'precip_mm', 'daylight']\n",
    "\n",
    "aeso_weather.describe(numeric_cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33f2483c-689c-4dd5-858e-de82c868dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------------+-----------+------------+\n",
      "|area_code|n_rows|avg_load_mw       |min_load_mw|max_load_mw |\n",
      "+---------+------+------------------+-----------+------------+\n",
      "|4        |10248 |27.30073670490833 |4.1753448  |55.2867376  |\n",
      "|6        |10248 |1117.9211637504688|764.6293749|1824.1138584|\n",
      "|13       |10248 |88.38278668826173 |50.8265535 |137.8933409 |\n",
      "|17       |10248 |12.041962755679217|2.60726    |47.3053119  |\n",
      "|18       |10248 |36.178930407572125|14.3762844 |53.6026496  |\n",
      "|19       |10248 |73.00554772053106 |50.07674   |106.47881   |\n",
      "|20       |10248 |280.4811968871972 |208.3936   |348.37712   |\n",
      "|21       |10248 |76.97281901053846 |33.95139   |101.16912   |\n",
      "|22       |10248 |54.940816131928045|11.06284   |69.33908    |\n",
      "|23       |10248 |23.719194094457514|9.89584    |35.2212     |\n",
      "|24       |10248 |82.96259741082159 |43.5704975 |99.157371   |\n",
      "|25       |10248 |688.6553386539241 |498.1173409|898.8798419 |\n",
      "|26       |10248 |197.20513409236   |66.9230553 |287.6762602 |\n",
      "|27       |10248 |110.88209880624532|79.2807079 |147.9809737 |\n",
      "|28       |10248 |127.26683576309458|88.0814929 |223.5134896 |\n",
      "|29       |10248 |102.52438759521836|63.6392381 |137.8930875 |\n",
      "|30       |10248 |122.26622213092291|94.3054757 |157.435448  |\n",
      "|31       |10248 |100.38363972461003|64.1659926 |153.0471403 |\n",
      "|32       |10248 |113.45360982818038|69.454435  |148.009989  |\n",
      "|33       |10248 |438.59636868573426|313.6239987|605.3644556 |\n",
      "+---------+------+------------------+-----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------------+-----------+-----------+\n",
      "|region_type|n_rows|avg_load_mw       |min_load_mw|max_load_mw|\n",
      "+-----------+------+------------------+-----------+-----------+\n",
      "|rural      |283368|112.59638268266158|0.0        |898.8798419|\n",
      "|urban      |143472|271.14317370887125|2.9328187  |2121.356533|\n",
      "+-----------+------+------------------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------------+-----------+------------+\n",
      "|hour|n_rows|avg_load_mw       |min_load_mw|max_load_mw |\n",
      "+----+------+------------------+-----------+------------+\n",
      "|0   |17785 |153.91109948992423|0.0        |1519.0256447|\n",
      "|1   |17785 |151.71338099371965|0.0        |1501.087999 |\n",
      "|2   |17743 |150.65805853058126|0.0        |1450.9684422|\n",
      "|3   |17827 |150.6630734374993 |0.0        |1451.388363 |\n",
      "|4   |17785 |152.13900690899646|0.0        |1461.1570141|\n",
      "|5   |17785 |156.26909920314864|0.0        |1498.7311083|\n",
      "|6   |17785 |162.30985143858317|0.0        |1591.7538483|\n",
      "|7   |17785 |166.98960630035432|0.0        |1719.9936441|\n",
      "|8   |17785 |168.98173228442508|0.0        |1803.3920744|\n",
      "|9   |17785 |169.82228544831605|0.0        |1894.9085384|\n",
      "|10  |17785 |170.71994723980333|0.0        |1962.4180801|\n",
      "|11  |17785 |171.09345309515876|0.0        |2001.2324488|\n",
      "|12  |17785 |171.24440111788016|0.0        |2052.9704715|\n",
      "|13  |17785 |171.24737067584493|0.0        |2082.5886139|\n",
      "|14  |17785 |171.78759438491429|0.0        |2102.694936 |\n",
      "|15  |17785 |173.4888012793984 |0.0        |2107.0080031|\n",
      "|16  |17785 |176.1981491963903 |0.0        |2121.356533 |\n",
      "|17  |17785 |177.2397126457351 |0.0        |2109.4010026|\n",
      "|18  |17785 |176.4618331122406 |0.0        |2044.423833 |\n",
      "|19  |17785 |175.42239113631155|0.0        |1989.6407283|\n",
      "+----+------+------------------+-----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------------------+-----------+------------+\n",
      "|dayofweek|n_rows|avg_load_mw       |min_load_mw|max_load_mw |\n",
      "+---------+------+------------------+-----------+------------+\n",
      "|1        |60984 |161.30989365334995|0.0        |2007.9945971|\n",
      "|2        |60960 |167.34958801058048|0.0        |2102.694936 |\n",
      "|3        |60960 |168.08662808255391|0.0        |2009.6570069|\n",
      "|4        |60984 |167.8324009797488 |0.0        |2121.356533 |\n",
      "|5        |60984 |167.69072080252528|0.0        |2107.0080031|\n",
      "|6        |60984 |167.0799176555013 |0.0        |2034.2196704|\n",
      "|7        |60984 |161.86880397666485|0.0        |1903.0245662|\n",
      "+---------+------+------------------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+-----------+------------+\n",
      "|month|n_rows|avg_load_mw       |min_load_mw|max_load_mw |\n",
      "+-----+------+------------------+-----------+------------+\n",
      "|1    |31248 |176.16515389315788|0.0        |1892.5002711|\n",
      "|2    |29232 |168.5755355338325 |0.0        |1629.8547005|\n",
      "|3    |31248 |164.68070042870283|0.0        |1621.8241125|\n",
      "|4    |30240 |157.6248427067362 |0.0        |1493.1059788|\n",
      "|5    |30624 |155.28632600689022|0.0        |1533.9919309|\n",
      "|6    |29520 |159.57220848143282|0.0        |1569.0155166|\n",
      "|7    |30504 |176.15465473888668|0.0        |2121.356533 |\n",
      "|8    |30504 |164.79456935265216|0.0        |1824.4598154|\n",
      "|9    |29520 |158.19389247204245|0.0        |1800.8761273|\n",
      "|10   |31224 |159.8397935854342 |0.0        |1569.5191787|\n",
      "|11   |60480 |168.05167523894215|0.0        |1726.0901032|\n",
      "|12   |62496 |172.35785841526993|0.0        |1782.333449 |\n",
      "+-----+------+------------------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+-----------+------------+\n",
      "|season|n_rows|avg_load_mw       |min_load_mw|max_load_mw |\n",
      "+------+------+------------------+-----------+------------+\n",
      "|fall  |121224|163.53598907095548|0.0        |1800.8761273|\n",
      "|spring|92112 |159.24099159808497|0.0        |1621.8241125|\n",
      "|summer|90528 |166.9194804487032 |0.0        |2121.356533 |\n",
      "|winter|122976|172.42620920422672|0.0        |1892.5002711|\n",
      "+------+------+------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nowe lets do categorical, NOW NOTE (even though month, day and hour represented by inetgers truly they are\n",
    "#categories as they can only be a finite value and month and day can be replaced by string i.e November or Saturday, so I chose to leave\n",
    "#them as categories due to what they represent:\n",
    "categorical_cols = [\"area_code\",\"region_type\", \"hour\", \"dayofweek\", \"month\", \"season\"]\n",
    "\n",
    "\n",
    "for column in categorical_cols:\n",
    "        stats = (\n",
    "        aeso_weather\n",
    "        .groupby(column)\n",
    "        .agg(F.count(\"*\").alias(\"n_rows\"),F.avg(\"load_mw\").alias(\"avg_load_mw\"),F.min(\"load_mw\").alias(\"min_load_mw\"),\n",
    "            F.max(\"load_mw\").alias(\"max_load_mw\")).orderBy(column))\n",
    "        stats.show(truncate=False) # if using DB i realized we display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8aba2500-ec8f-420a-b7cb-05f84e830647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hmmmm lots of good insights here, will discuss later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2111929-aca0-4c2c-b685-f8b419750030",
   "metadata": {},
   "source": [
    "# Modelling Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8662d-50c0-4ed5-9536-3f4e5487c272",
   "metadata": {},
   "source": [
    "# Feature Columns Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e034d61-3c8a-4bd8-8d59-6f383644df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first set up where everything is\n",
    "\n",
    "#realized that even thoigh area code is a int it's really a category as it respresents an area\n",
    "#so we have to convert it to a string (its also not complete)\n",
    "\n",
    "aeso_weather = aeso_weather.withColumn(\"area_code_str\", col(\"area_code\").cast(\"string\"))\n",
    "\n",
    "numeric_cols = [\"temp_c\", \"precip_mm\", \"daylight\"]\n",
    "\n",
    "categorical_cols = [\"region_type\",\"season\",\"hour\",\"dayofweek\",\"month\",\"area_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d81cf270-dfc4-4362-94e1-84b613959b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "indexer = StringIndexer(inputCols= categorical_cols, outputCols= [column + \"Index\" for column in categorical_cols] )\n",
    "encoder = OneHotEncoder(inputCols = [column + \"Index\" for column in categorical_cols], outputCols = [column + \"Encoder\" for column in categorical_cols])\n",
    "\n",
    "\n",
    "indexerModel = indexer.fit(aeso_weather)\n",
    "aeso_weather = indexerModel.transform(aeso_weather)\n",
    "encoderModel = encoder.fit(aeso_weather)\n",
    "aeso_weather = encoderModel.transform(aeso_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4a90e8d6-8f79-4485-8cc1-ca24c6407d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['temp_c', 'precip_mm', 'daylight', 'region_typeEncoder', 'seasonEncoder', 'hourEncoder', 'dayofweekEncoder', 'monthEncoder', 'area_codeEncoder']\n",
      "['temp_c', 'precip_mm', 'daylight', 'region_typeEncoder', 'seasonEncoder', 'hourEncoder', 'dayofweekEncoder', 'monthEncoder', 'area_codeEncoder']\n"
     ]
    }
   ],
   "source": [
    "#Awesome time this, again great reference to this week 7 and 8 notes, esepcially in machine-learning.iypmb you provided\n",
    "\n",
    "#now need our input columns, so will use my already defined numerical array and than pull these encoded columns (output from encoder, which I will just do manually)\n",
    "\n",
    "combineInputColumns = numeric_cols + [column + \"Encoder\" for column in categorical_cols]\n",
    "\n",
    "print(combineInputColumns)\n",
    "\n",
    "#Excellent that all looks good!, so can use our assembler\n",
    "#Actually have to remove price from that, good catch\n",
    "\n",
    "combineInputColumnsGood = [c for c in combineInputColumns if c != \"load_mw\"]\n",
    "print(combineInputColumns)\n",
    "\n",
    "#great assembler time\n",
    "\n",
    "assembler = VectorAssembler(inputCols=combineInputColumnsGood, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "aeso_weather_Assembled = assembler.transform(aeso_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a758e8-2c70-4bae-8670-051590d4da1d",
   "metadata": {},
   "source": [
    "Beautiful that looks great all we have left is our Numeric columns of Temp and Precip and the rest are the encoded vectorized feature columns with the original (category columns removed as they have now been vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c0cced4a-25c0-427a-8530-22545954474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol = \"features_raw\", outputCol = \"features\", withMean = False, withStd = True)\n",
    "scalerModel = scaler.fit(aeso_weather_Assembled)\n",
    "aeso_weather_Scaled = scalerModel.transform(aeso_weather_Assembled)\n",
    "aeso_weather_Final = aeso_weather_Scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebaca0e-fad0-4f76-9f88-a97cedc971eb",
   "metadata": {},
   "source": [
    "# NOW THAT WE HAVE FINAL DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a58dc936-d48d-484d-8318-314b21c9fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to build our test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4d96aac8-5e21-410a-9039-039e8d6681d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = aeso_weather_Final.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3b2c03d1-6bfc-45de-ae47-55ff82c4d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 416592, (97.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 333695, (80.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5953:>                                                     (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rows: 82897, (19.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df = aeso_weather_Final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "#COPIED MY CODE FROM ASSIGNMENT 2 that I wrote to get the calculations\n",
    "#Now just doing some calculation and holding them in variables not match the loaded ouput\n",
    "\n",
    "totalRowsPercentage = aeso_weather_Final.count() / aeso_weather.count() * 100\n",
    "traindDFRowsPercentage = train_df.count() / aeso_weather_Final.count() * 100\n",
    "testDFRowsPercentage = test_df.count() / aeso_weather_Final.count() * 100\n",
    "\n",
    "print(f\"Total rows: {aeso_weather_Final.count()}, ({totalRowsPercentage:.2f}%)\")\n",
    "print(f\"Training rows: {train_df.count()}, ({traindDFRowsPercentage:.2f}%)\")\n",
    "print(f\"Testing rows: {test_df.count()}, ({testDFRowsPercentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e6d22a2d-a8e6-4ec6-a4ba-0585850c8fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 09:54:14 WARN DAGScheduler: Broadcasting large task binary with size 1059.2 KiB\n",
      "25/12/05 09:54:15 WARN DAGScheduler: Broadcasting large task binary with size 1190.9 KiB\n",
      "25/12/05 09:54:15 WARN DAGScheduler: Broadcasting large task binary with size 1335.9 KiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decision_tree = DecisionTreeRegressor(labelCol=\"load_mw\", featuresCol=\"features\", predictionCol = \"prediction\", maxDepth =20, minInstancesPerNode = 10)\n",
    "decisionTreeModel = decision_tree.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6526083e-ea2b-47dd-821a-e888346577ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNonPipeline = decisionTreeModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7fea2d2f-4f3e-4a6d-8e19-94926b1142a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6104:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+----------+-----------+------------------+----+---------+-----+------+------+---------+--------+-------------+----------------+-----------+---------+--------------+----------+--------------+------------------+-------------+---------------+----------------+--------------+----------------+--------------------+--------------------+------------------+\n",
      "|          timestamp|area_code|area_name|   load_mw|region_type|     location_name|hour|dayofweek|month|season|temp_c|precip_mm|daylight|area_code_str|region_typeIndex|seasonIndex|hourIndex|dayofweekIndex|monthIndex|area_codeIndex|region_typeEncoder|seasonEncoder|    hourEncoder|dayofweekEncoder|  monthEncoder|area_codeEncoder|        features_raw|            features|        prediction|\n",
      "+-------------------+---------+---------+----------+-----------+------------------+----+---------+-----+------+------+---------+--------+-------------+----------------+-----------+---------+--------------+----------+--------------+------------------+-------------+---------------+----------------+--------------+----------------+--------------------+--------------------+------------------+\n",
      "|2023-11-01 00:00:00|       49|   AREA49|  9.393875|      rural|           Stavely|   0|        4|   11|  fall|  -5.1|      0.0|       0|           49|             0.0|        1.0|      1.0|           1.0|       1.0|          32.0|     (1,[0],[1.0])|(3,[1],[1.0])| (23,[1],[1.0])|   (6,[1],[1.0])|(11,[1],[1.0])| (41,[32],[1.0])|(88,[0,3,5,8,31,3...|(88,[0,3,5,8,31,3...|14.421533918656342|\n",
      "|2023-11-01 02:00:00|       53|   AREA53|11.3977542|      urban|      Fort Macleod|   2|        4|   11|  fall|  -7.4|      0.0|       0|           53|             1.0|        1.0|     23.0|           1.0|       1.0|          34.0|         (1,[],[])|(3,[1],[1.0])|     (23,[],[])|   (6,[1],[1.0])|(11,[1],[1.0])| (41,[34],[1.0])|(88,[0,5,31,37,81...|(88,[0,5,31,37,81...| 19.62312578647055|\n",
      "|2023-11-01 03:00:00|       46|   AREA46|43.1809826|      urban|        High River|   3|        4|   11|  fall|  -8.9|      0.0|       0|           46|             1.0|        1.0|      0.0|           1.0|       1.0|          29.0|         (1,[],[])|(3,[1],[1.0])| (23,[0],[1.0])|   (6,[1],[1.0])|(11,[1],[1.0])| (41,[29],[1.0])|(88,[0,5,7,31,37,...|(88,[0,5,7,31,37,...| 64.30154343448073|\n",
      "|2023-11-01 04:00:00|       45|   AREA45| 68.992275|      urban|Strathmore/Blackie|   4|        4|   11|  fall|  -4.9|      0.0|       0|           45|             1.0|        1.0|     17.0|           1.0|       1.0|          28.0|         (1,[],[])|(3,[1],[1.0])|(23,[17],[1.0])|   (6,[1],[1.0])|(11,[1],[1.0])| (41,[28],[1.0])|(88,[0,5,24,31,37...|(88,[0,5,24,31,37...| 64.30154343448073|\n",
      "|2023-11-01 07:00:00|       31|   AREA31|118.521359|      urban|        Wetaskiwin|   7|        4|   11|  fall|  -3.0|      0.0|       0|           31|             1.0|        1.0|     20.0|           1.0|       1.0|          15.0|         (1,[],[])|(3,[1],[1.0])|(23,[20],[1.0])|   (6,[1],[1.0])|(11,[1],[1.0])| (41,[15],[1.0])|(88,[0,5,27,31,37...|(88,[0,5,27,31,37...| 64.30154343448073|\n",
      "+-------------------+---------+---------+----------+-----------+------------------+----+---------+-----+------+------+---------+--------+-------------+----------------+-----------+---------+--------------+----------+--------------+------------------+-------------+---------------+----------------+--------------+----------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionsNonPipeline.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36d2cbf7-d196-4803-b8d0-0eda2c3c2660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'area_code', 'area_name', 'load_mw', 'region_type', 'location_name', 'hour', 'dayofweek', 'month', 'season', 'temp_c', 'precip_mm', 'daylight', 'area_code_str', 'region_typeIndex', 'seasonIndex', 'hourIndex', 'dayofweekIndex', 'monthIndex', 'area_codeIndex', 'region_typeEncoder', 'seasonEncoder', 'hourEncoder', 'dayofweekEncoder', 'monthEncoder', 'area_codeEncoder', 'features_raw', 'features']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6115:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----------+------------------+\n",
      "|timestamp          |area_name|load_mw   |prediction        |\n",
      "+-------------------+---------+----------+------------------+\n",
      "|2023-11-01 00:00:00|AREA49   |9.393875  |14.421533918656342|\n",
      "|2023-11-01 02:00:00|AREA53   |11.3977542|19.62312578647055 |\n",
      "|2023-11-01 03:00:00|AREA46   |43.1809826|64.30154343448073 |\n",
      "|2023-11-01 04:00:00|AREA45   |68.992275 |64.30154343448073 |\n",
      "|2023-11-01 07:00:00|AREA31   |118.521359|64.30154343448073 |\n",
      "|2023-11-01 09:00:00|AREA21   |69.40392  |64.30154343448073 |\n",
      "|2023-11-01 10:00:00|AREA53   |12.4471908|19.62312578647055 |\n",
      "|2023-11-01 14:00:00|AREA24   |80.2330875|64.30154343448073 |\n",
      "|2023-11-01 18:00:00|AREA13   |93.4458097|64.30154343448073 |\n",
      "|2023-11-01 18:00:00|AREA17   |5.72264   |13.823112301117026|\n",
      "+-------------------+---------+----------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make sure columns look right\n",
    "print(train_df.columns)\n",
    "\n",
    "# Compare true vs predicted loads\n",
    "predictionsNonPipeline.select(\n",
    "    \"timestamp\", \"area_name\", \"load_mw\", \"prediction\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abaa4c-ae33-4792-a8b0-f48ee112fbb4",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5c915a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 09:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1033.4 KiB\n",
      "25/12/05 09:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1030.1 KiB\n",
      "25/12/05 09:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1030.6 KiB\n",
      "25/12/05 09:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1031.2 KiB\n",
      "25/12/05 09:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1032.4 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1034.8 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1039.4 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1048.5 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1065.4 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1096.3 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1150.5 KiB\n",
      "25/12/05 09:54:35 WARN DAGScheduler: Broadcasting large task binary with size 1149.3 KiB\n",
      "25/12/05 09:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1149.8 KiB\n",
      "25/12/05 09:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1150.4 KiB\n",
      "25/12/05 09:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1151.6 KiB\n",
      "25/12/05 09:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1153.9 KiB\n",
      "25/12/05 09:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1158.6 KiB\n",
      "25/12/05 09:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1168.0 KiB\n",
      "25/12/05 09:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1186.8 KiB\n",
      "25/12/05 09:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1221.3 KiB\n",
      "25/12/05 09:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1281.0 KiB\n",
      "25/12/05 09:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1276.9 KiB\n",
      "25/12/05 09:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1277.4 KiB\n",
      "25/12/05 09:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1278.0 KiB\n",
      "25/12/05 09:54:38 WARN DAGScheduler: Broadcasting large task binary with size 1279.2 KiB\n",
      "25/12/05 09:54:38 WARN DAGScheduler: Broadcasting large task binary with size 1281.6 KiB\n",
      "25/12/05 09:54:38 WARN DAGScheduler: Broadcasting large task binary with size 1286.3 KiB\n",
      "25/12/05 09:54:38 WARN DAGScheduler: Broadcasting large task binary with size 1295.4 KiB\n",
      "25/12/05 09:54:38 WARN DAGScheduler: Broadcasting large task binary with size 1312.3 KiB\n",
      "25/12/05 09:54:39 WARN DAGScheduler: Broadcasting large task binary with size 1343.5 KiB\n",
      "25/12/05 09:54:39 WARN DAGScheduler: Broadcasting large task binary with size 1398.2 KiB\n",
      "25/12/05 09:54:39 WARN DAGScheduler: Broadcasting large task binary with size 1396.7 KiB\n",
      "25/12/05 09:54:39 WARN DAGScheduler: Broadcasting large task binary with size 1397.2 KiB\n",
      "25/12/05 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 1397.7 KiB\n",
      "25/12/05 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 1398.9 KiB\n",
      "25/12/05 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 1401.3 KiB\n",
      "25/12/05 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 1406.0 KiB\n",
      "25/12/05 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 1415.4 KiB\n",
      "25/12/05 09:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1433.2 KiB\n",
      "25/12/05 09:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1466.9 KiB\n",
      "25/12/05 09:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1522.1 KiB\n",
      "25/12/05 09:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1517.7 KiB\n",
      "25/12/05 09:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1518.2 KiB\n",
      "25/12/05 09:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1518.7 KiB\n",
      "25/12/05 09:54:42 WARN DAGScheduler: Broadcasting large task binary with size 1520.0 KiB\n",
      "25/12/05 09:54:42 WARN DAGScheduler: Broadcasting large task binary with size 1522.3 KiB\n",
      "25/12/05 09:54:42 WARN DAGScheduler: Broadcasting large task binary with size 1527.0 KiB\n",
      "25/12/05 09:54:42 WARN DAGScheduler: Broadcasting large task binary with size 1536.1 KiB\n",
      "25/12/05 09:54:42 WARN DAGScheduler: Broadcasting large task binary with size 1553.1 KiB\n",
      "25/12/05 09:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1584.5 KiB\n",
      "25/12/05 09:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1638.7 KiB\n",
      "25/12/05 09:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1636.9 KiB\n",
      "25/12/05 09:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1637.4 KiB\n",
      "25/12/05 09:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1638.0 KiB\n",
      "25/12/05 09:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1639.2 KiB\n",
      "25/12/05 09:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1641.6 KiB\n",
      "25/12/05 09:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1646.3 KiB\n",
      "25/12/05 09:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1655.6 KiB\n",
      "25/12/05 09:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1673.7 KiB\n",
      "25/12/05 09:54:45 WARN DAGScheduler: Broadcasting large task binary with size 1706.6 KiB\n",
      "25/12/05 09:54:45 WARN DAGScheduler: Broadcasting large task binary with size 1764.3 KiB\n",
      "25/12/05 09:54:45 WARN DAGScheduler: Broadcasting large task binary with size 1761.4 KiB\n",
      "25/12/05 09:54:45 WARN DAGScheduler: Broadcasting large task binary with size 1761.8 KiB\n",
      "25/12/05 09:54:45 WARN DAGScheduler: Broadcasting large task binary with size 1762.4 KiB\n",
      "25/12/05 09:54:46 WARN DAGScheduler: Broadcasting large task binary with size 1763.6 KiB\n",
      "25/12/05 09:54:46 WARN DAGScheduler: Broadcasting large task binary with size 1766.0 KiB\n",
      "25/12/05 09:54:46 WARN DAGScheduler: Broadcasting large task binary with size 1770.7 KiB\n",
      "25/12/05 09:54:46 WARN DAGScheduler: Broadcasting large task binary with size 1780.1 KiB\n",
      "25/12/05 09:54:46 WARN DAGScheduler: Broadcasting large task binary with size 1797.6 KiB\n",
      "25/12/05 09:54:47 WARN DAGScheduler: Broadcasting large task binary with size 1829.7 KiB\n",
      "25/12/05 09:54:47 WARN DAGScheduler: Broadcasting large task binary with size 1885.0 KiB\n",
      "25/12/05 09:54:47 WARN DAGScheduler: Broadcasting large task binary with size 1881.6 KiB\n",
      "25/12/05 09:54:47 WARN DAGScheduler: Broadcasting large task binary with size 1882.1 KiB\n",
      "25/12/05 09:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1882.6 KiB\n",
      "25/12/05 09:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1883.8 KiB\n",
      "25/12/05 09:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1886.2 KiB\n",
      "25/12/05 09:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1890.9 KiB\n",
      "25/12/05 09:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1900.3 KiB\n",
      "25/12/05 09:54:49 WARN DAGScheduler: Broadcasting large task binary with size 1919.0 KiB\n",
      "25/12/05 09:54:49 WARN DAGScheduler: Broadcasting large task binary with size 1952.9 KiB\n",
      "25/12/05 09:54:49 WARN DAGScheduler: Broadcasting large task binary with size 2009.2 KiB\n",
      "25/12/05 09:54:49 WARN DAGScheduler: Broadcasting large task binary with size 2003.1 KiB\n",
      "25/12/05 09:54:50 WARN DAGScheduler: Broadcasting large task binary with size 2003.5 KiB\n",
      "25/12/05 09:54:50 WARN DAGScheduler: Broadcasting large task binary with size 2004.1 KiB\n",
      "25/12/05 09:54:50 WARN DAGScheduler: Broadcasting large task binary with size 2005.3 KiB\n",
      "25/12/05 09:54:50 WARN DAGScheduler: Broadcasting large task binary with size 2007.7 KiB\n",
      "25/12/05 09:54:50 WARN DAGScheduler: Broadcasting large task binary with size 2012.4 KiB\n",
      "25/12/05 09:54:51 WARN DAGScheduler: Broadcasting large task binary with size 2021.2 KiB\n",
      "25/12/05 09:54:51 WARN DAGScheduler: Broadcasting large task binary with size 2037.9 KiB\n",
      "25/12/05 09:54:51 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/05 09:54:51 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:51 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:53 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:53 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:53 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:53 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/05 09:54:53 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/05 09:54:56 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:58 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/05 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/05 09:55:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/05 09:55:05 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:05 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:07 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:07 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:07 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:08 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:08 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/05 09:55:08 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/05 09:55:09 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/05 09:55:09 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:09 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:10 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:10 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:10 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/05 09:55:12 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:13 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:13 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:13 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:14 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:14 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:14 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:15 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:15 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:15 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/05 09:55:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:18 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:18 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/12/05 09:55:18 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/05 09:55:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/12/05 09:55:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/12/05 09:55:25 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/12/05 09:55:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/12/05 09:55:26 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:26 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:27 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:27 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:28 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:28 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:29 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:29 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:29 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:30 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/05 09:55:30 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:32 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:32 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:33 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:33 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:33 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:34 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/12/05 09:55:34 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:34 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:35 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:35 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:35 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:36 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:36 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:36 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:37 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:37 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/12/05 09:55:38 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:38 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:38 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:39 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:39 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:40 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:40 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:41 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:41 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/12/05 09:55:41 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:42 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:42 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:43 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:43 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:44 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:44 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:45 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:45 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:46 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/12/05 09:55:46 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:47 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:47 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:48 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:48 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:50 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:50 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/05 09:55:51 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:51 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:52 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:52 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:52 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:54 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:54 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/12/05 09:55:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/12/05 09:55:58 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:55:58 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:55:59 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:55:59 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:56:00 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:56:00 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:56:01 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:56:01 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:56:02 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/12/05 09:56:02 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:03 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:03 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:04 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:04 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:05 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:05 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:06 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:06 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/12/05 09:56:07 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/12/05 09:56:07 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/12/05 09:56:08 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/12/05 09:56:08 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/12/05 09:56:09 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:09 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:10 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:10 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:11 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:11 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:11 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:12 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:13 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:13 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/12/05 09:56:13 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:14 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:14 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:15 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:15 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:16 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:16 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:17 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:17 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:18 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/12/05 09:56:18 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:19 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:19 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:20 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:20 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:21 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:21 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:22 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:22 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:23 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/12/05 09:56:23 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:24 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:25 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:25 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:26 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:26 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:27 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:27 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:28 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:28 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/12/05 09:56:29 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:29 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:30 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:30 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:31 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:31 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:32 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:32 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:33 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/12/05 09:56:33 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:34 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:35 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:35 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:36 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:36 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:37 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:37 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:38 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:38 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/12/05 09:56:39 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:39 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:40 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:41 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:41 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:42 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:42 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:43 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:43 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:44 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/12/05 09:56:44 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:45 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:46 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:46 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:47 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:47 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:48 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:48 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:49 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:49 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/12/05 09:56:50 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:50 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:51 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:52 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:52 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:53 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:53 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:54 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:54 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:55 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/12/05 09:56:55 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:56 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:57 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:57 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:58 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:58 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:59 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:56:59 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:57:00 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:57:00 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/12/05 09:57:01 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:02 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:02 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:03 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:03 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:04 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:05 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:05 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:06 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/12/05 09:57:06 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:07 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:07 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:08 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:09 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:09 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:10 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:10 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:11 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:11 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/12/05 09:57:12 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:13 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:13 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:14 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:15 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:15 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:16 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:16 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:17 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/12/05 09:57:17 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:18 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:19 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:19 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:20 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:21 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:21 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:22 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:22 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:23 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/12/05 09:57:23 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/12/05 09:57:24 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/12/05 09:57:25 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/12/05 09:57:25 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Model\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Create and train Gradient Boosting model\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=\"load_mw\", \n",
    "    featuresCol=\"features\", \n",
    "    predictionCol=\"prediction\",\n",
    "    maxDepth=10,\n",
    "    maxIter=50,\n",
    "    stepSize=0.1\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "215c4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9186:>                                                     (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRADIENT BOOSTING MODEL EVALUATION\n",
      "============================================================\n",
      "RMSE (Root Mean Squared Error): 19.4870\n",
      "MAE (Mean Absolute Error): 10.3238\n",
      "RÂ² (Coefficient of Determination): 0.9950\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Model Evaluation - Gradient Boosting\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"load_mw\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Evaluate Gradient Boosting model\n",
    "gbt_rmse = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"rmse\"})\n",
    "gbt_mae = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"mae\"})\n",
    "gbt_r2 = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRADIENT BOOSTING MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"RMSE (Root Mean Squared Error): {gbt_rmse:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {gbt_mae:.4f}\")\n",
    "print(f\"RÂ² (Coefficient of Determination): {gbt_r2:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "69ef75ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9222:>                                                     (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DECISION TREE MODEL EVALUATION\n",
      "============================================================\n",
      "RMSE (Root Mean Squared Error): 25.5989\n",
      "MAE (Mean Absolute Error): 17.9671\n",
      "RÂ² (Coefficient of Determination): 0.9914\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate Decision Tree model for comparison\n",
    "dt_rmse = evaluator.evaluate(predictionsNonPipeline, {evaluator.metricName: \"rmse\"})\n",
    "dt_mae = evaluator.evaluate(predictionsNonPipeline, {evaluator.metricName: \"mae\"})\n",
    "dt_r2 = evaluator.evaluate(predictionsNonPipeline, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DECISION TREE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"RMSE (Root Mean Squared Error): {dt_rmse:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {dt_mae:.4f}\")\n",
    "print(f\"RÂ² (Coefficient of Determination): {dt_r2:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "00181c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "Metric                         Decision Tree        Gradient Boosting   \n",
      "------------------------------------------------------------\n",
      "RMSE                           25.5989              19.4870             \n",
      "MAE                            17.9671              10.3238             \n",
      "RÂ²                             0.9914               0.9950              \n",
      "============================================================\n",
      "\n",
      "âœ“ Gradient Boosting has lower RMSE (better)\n",
      "âœ“ Gradient Boosting has higher RÂ² (better)\n"
     ]
    }
   ],
   "source": [
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Decision Tree':<20} {'Gradient Boosting':<20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'RMSE':<30} {dt_rmse:<20.4f} {gbt_rmse:<20.4f}\")\n",
    "print(f\"{'MAE':<30} {dt_mae:<20.4f} {gbt_mae:<20.4f}\")\n",
    "print(f\"{'RÂ²':<30} {dt_r2:<20.4f} {gbt_r2:<20.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine which model performs better\n",
    "if gbt_rmse < dt_rmse:\n",
    "    print(\"\\nâœ“ Gradient Boosting has lower RMSE (better)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Decision Tree has lower RMSE (better)\")\n",
    "\n",
    "if gbt_r2 > dt_r2:\n",
    "    print(\"âœ“ Gradient Boosting has higher RÂ² (better)\")\n",
    "else:\n",
    "    print(\"âœ“ Decision Tree has higher RÂ² (better)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
