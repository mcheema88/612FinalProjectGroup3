{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set up paths\n",
    "data_dir = Path(\"../data\")  # Adjust if your CSV files are in a different directory\n",
    "mapping_file = data_dir / \"area_region_mapping.csv\"\n",
    "\n",
    "# Load the area mapping\n",
    "area_mapping = pd.read_csv(mapping_file)\n",
    "print(\"Area mapping loaded:\")\n",
    "print(area_mapping.head())\n",
    "print(f\"\\nTotal areas in mapping: {len(area_mapping)}\")\n",
    "\n",
    "# Get list of all area numbers from mapping (excluding regional aggregates)\n",
    "area_codes = area_mapping[area_mapping['area_code'].str.isdigit()]['area_code'].astype(int).tolist()\n",
    "print(f\"\\nArea codes to process: {sorted(area_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine all area CSV files\n",
    "def combine_area_dataframes(data_directory, area_codes, area_mapping):\n",
    "    \"\"\"\n",
    "    Combines multiple area CSV files into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_directory: Path to directory containing CSV files\n",
    "    - area_codes: List of area codes to process\n",
    "    - area_mapping: DataFrame with area metadata\n",
    "    \n",
    "    Returns:\n",
    "    - combined_df: Single DataFrame with all area data\n",
    "    \"\"\"\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    # Loop through each area code\n",
    "    for area_code in area_codes:\n",
    "        # Look for CSV files with pattern: \"{area_code}_combine_data.csv\"\n",
    "        csv_pattern = f\"{area_code}_combine_data.csv\"\n",
    "        csv_file = data_directory / csv_pattern\n",
    "        \n",
    "        if csv_file.exists():\n",
    "            print(f\"Processing {csv_file.name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Add area metadata\n",
    "                area_info = area_mapping[area_mapping['area_code'] == str(area_code)].iloc[0]\n",
    "                df['area_code'] = area_code\n",
    "                df['region_type'] = area_info['region_type']\n",
    "                df['location_name'] = area_info['location_name']\n",
    "                \n",
    "                # Add to combined data\n",
    "                combined_data.append(df)\n",
    "                print(f\"  - Added {len(df)} rows for Area {area_code} ({area_info['location_name']})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  - Error processing {csv_file.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {csv_pattern}\")\n",
    "    \n",
    "    if combined_data:\n",
    "        # Combine all DataFrames\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        print(f\"\\nCombination complete!\")\n",
    "        print(f\"Total rows: {len(combined_df):,}\")\n",
    "        print(f\"Total areas processed: {len(combined_data)}\")\n",
    "        print(f\"Columns: {list(combined_df.columns)}\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data files found to combine!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute the combination\n",
    "combined_df = combine_area_dataframes(data_dir, area_codes, area_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks and summary\n",
    "if not combined_df.empty:\n",
    "    print(\"=== DATA QUALITY SUMMARY ===\")\n",
    "    print(f\"Shape: {combined_df.shape}\")\n",
    "    print(f\"Date range: {combined_df['datetime'].min()} to {combined_df['datetime'].max()}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values:\")\n",
    "    missing_counts = combined_df.isnull().sum()\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    \n",
    "    # Regional summary\n",
    "    print(f\"\\nRegional breakdown:\")\n",
    "    regional_summary = combined_df.groupby(['region_type', 'location_name']).size().reset_index(name='row_count')\n",
    "    print(regional_summary.to_string(index=False))\n",
    "    \n",
    "    # Urban vs Rural summary\n",
    "    print(f\"\\nUrban vs Rural summary:\")\n",
    "    urban_rural_summary = combined_df.groupby('region_type').agg({\n",
    "        'area_code': 'nunique',\n",
    "        'load_mw': ['count', 'mean', 'std'],\n",
    "        'temperature_c': 'mean'\n",
    "    }).round(2)\n",
    "    print(urban_rural_summary)\n",
    "    \n",
    "    # Sample of combined data\n",
    "    print(f\"\\nSample of combined dataset:\")\n",
    "    print(combined_df.head(10).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"No data to analyze - combined DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e515e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset\n",
    "if not combined_df.empty:\n",
    "    # Save as CSV\n",
    "    output_file = data_dir / \"combined_all_areas.csv\"\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined dataset saved to: {output_file}\")\n",
    "    \n",
    "    # Save as Parquet for better performance (optional)\n",
    "    try:\n",
    "        parquet_file = data_dir / \"combined_all_areas.parquet\"\n",
    "        combined_df.to_parquet(parquet_file, index=False)\n",
    "        print(f\"Also saved as Parquet: {parquet_file}\")\n",
    "    except ImportError:\n",
    "        print(\"Parquet not available - install pyarrow or fastparquet for better performance\")\n",
    "    \n",
    "    # Create separate urban and rural datasets\n",
    "    urban_df = combined_df[combined_df['region_type'] == 'urban'].copy()\n",
    "    rural_df = combined_df[combined_df['region_type'] == 'rural'].copy()\n",
    "    \n",
    "    if len(urban_df) > 0:\n",
    "        urban_file = data_dir / \"urban_areas_combined.csv\"\n",
    "        urban_df.to_csv(urban_file, index=False)\n",
    "        print(f\"Urban dataset saved: {urban_file} ({len(urban_df):,} rows)\")\n",
    "    \n",
    "    if len(rural_df) > 0:\n",
    "        rural_file = data_dir / \"rural_areas_combined.csv\" \n",
    "        rural_df.to_csv(rural_file, index=False)\n",
    "        print(f\"Rural dataset saved: {rural_file} ({len(rural_df):,} rows)\")\n",
    "    \n",
    "    print(f\"\\nData combination complete!\")\n",
    "    print(f\"Ready for machine learning model development\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to save - please check that CSV files exist in the correct format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
